# ğŸŒ AI Sensei: Multi-Modal Japanese Language Tutor

A cutting-edge **closed-loop AI tutor** that combines three specialized AI models to teach Japanese through **handwriting recognition**, **pronunciation analysis**, and **intelligent conversation**.

## ğŸŒŸ Key Features

### **The Eyes ğŸ‘ï¸** - Handwriting Recognition
- Custom CNN trained on **KMNIST** (49 Hiragana characters)
- Real-time character detection from canvas drawings
- Stroke-aware feedback for improved writing

### **The Ears ğŸ‘‚** - Pronunciation Scoring  
- Wav2Vec2 speech recognition with phoneme-level analysis
- **Levenshtein distance** for precise error detection
- Identifies exactly which sounds need improvement

### **The Brain ğŸ§ ** - Intelligent Curriculum
- GPT-4o/Gemini LLM orchestrator
- **Dynamic prompt injection** with sensor data
- Grounded feedback that's specific, not generic

### **Conversation Mode ğŸ—£ï¸** - Real Dialogue
- Text-to-speech (Microsoft Edge TTS) for natural Japanese pronunciation
- Speech recognition for student responses
- Back-and-forth conversation with 4 built-in scenarios
- Pre-defined dialogues: Greetings, Shopping, Restaurant, Directions

---

## ğŸš€ Quick Start

### Requirements
- Python 3.8+
- Jupyter Notebook or Google Colab
- GPU (optional, but recommended for faster inference)

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/ai-sensei.git
cd ai-sensei

# Install dependencies
pip install -r requirements.txt
```

### Running the Notebook

```bash
jupyter notebook ai_tutor.ipynb
```

Or open in **Google Colab**:
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/ai-sensei/blob/main/ai_tutor.ipynb)

---

## ğŸ“š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend UI (Jupyter)                â”‚
â”‚              Canvas Drawing + Audio Recording           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚The Eyesâ”‚      â”‚The Earsâ”‚      â”‚The Brainâ”‚
    â”‚(CNN)   â”‚      â”‚Wav2Vec2â”‚      â”‚(LLM)   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              Personalized Feedback
                  (Grounded in Data)
```

### Component Details

| Component | Model | Framework | Purpose |
|-----------|-------|-----------|---------|
| **Vision** | SenseiVisionNet | PyTorch + KMNIST | Handwriting â†’ Character classification |
| **Audio** | Wav2Vec2 + CTC | HuggingFace Transformers | Speech â†’ Phoneme alignment + scoring |
| **LLM** | GPT-4o/Gemini 1.5 | OpenAI/Google APIs | Sensor data â†’ Pedagogical response |
| **UI** | HTML/JS Canvas | Jupyter | Drawing & recording capture |

---

## ğŸ® Usage Guide

### 1. **Interactive Lesson Mode**

```python
# Start a lesson
sensei = AISenseiLesson(student_profile, CURRICULUM)
sensei.display_lesson_ui()

# Record handwriting + pronunciation
# â†’ AI analyzes â†’ Provides feedback
```

### 2. **Conversation Mode**

```python
# Choose a scenario
SELECTED_SCENARIO = 'greetings'  # or 'shopping', 'restaurant', 'directions'

# Start conversation
conversation = ConversationSession(SELECTED_SCENARIO)
conversation.start_conversation()

# Record response â†’ Get scored â†’ Continue
```

### 3. **Demo Mode** (No hardware needed)

```python
# Test the complete system
demo_complete_loop()

# Simulates sensor data to show closed-loop feedback
```

---

## ğŸ“Š Example Workflow

### Interactive Conversation Session

```
ğŸ¤– AI Sensei: "ã“ã‚“ã«ã¡ã¯ï¼"
             (Konnichiwa! / Hello!)

ğŸ‘¤ You: [Press ğŸ™ï¸ to record] "ã“ã‚“ã«ã¡ã¯"

ğŸ“Š Analysis:
   - Audio Score: 87/100
   - Detected: "konnichiwa" âœ… (Perfect!)
   
ğŸ§  AI Response:
   "ç´ æ™´ã‚‰ã—ã„! (Subarashii!) Excellent pronunciation!
    Now let me ask you: ãŠå…ƒæ°—ã§ã™ã‹? (How are you?)"
```

---

## ğŸ¯ Conversation Scenarios

### Beginner
- **Daily Greetings** (3 exchanges)
  - Greeting exchange
  - Health check
  - Name introduction

### Intermediate
- **At the Store** (3 exchanges)
- **At a Restaurant** (3 exchanges)
- **Asking for Directions** (2 exchanges)

---

## âš™ï¸ Configuration

### Set Your API Key

In the notebook, cell "Configure LLM API":

```python
# OpenAI
os.environ["OPENAI_API_KEY"] = "your-key-here"

# Or Google Gemini
os.environ["GOOGLE_API_KEY"] = "your-key-here"
```

### Customize Lesson Curriculum

Edit `CURRICULUM` dictionary:

```python
CURRICULUM = {
    'beginner': {
        'name': 'Hiragana Basics',
        'lessons': [
            {'phrase': 'ã‚', 'romaji': 'a', 'meaning': 'Vowel A'},
            # Add more lessons...
        ]
    }
}
```

---

## ğŸ”§ Technical Details

### Model Architecture

**SenseiVisionNet (CNN)**
```
Input (28Ã—28) 
  â†’ Conv2d(32) + BatchNorm + ReLU + MaxPool
  â†’ Conv2d(64) + BatchNorm + ReLU + MaxPool
  â†’ Conv2d(128) + BatchNorm + ReLU
  â†’ Dense(256) + Dropout
  â†’ Dense(128) + Dropout
  â†’ Output(49 classes)
```

**Pronunciation Scoring**
```
Audio â†’ Wav2Vec2 â†’ Transcription
      â†’ Romaji Conversion â†’ Levenshtein Distance
      â†’ Score (0-100) + Error Indices
```

**LLM Prompt Injection**
```
System Prompt:
  "You are AI Sensei..."
  "SENSOR DATA:"
  "- Handwriting: 72/100"
  "- Pronunciation: 58/100"
  "- Errors at positions: [3, 4]"
  
Response: Specific, grounded feedback
```

---

## ğŸ“¦ Dependencies

See `requirements.txt` for complete list:
- **Deep Learning**: torch, torchvision, transformers
- **Audio**: librosa, soundfile, edge-tts, pykakasi
- **LLM APIs**: openai, google-generativeai
- **Utilities**: numpy, pandas, matplotlib, ipywidgets

---

## ğŸš¨ Troubleshooting

### "NameError: name 'nn' is not defined"
â†’ Run cell 4 (Import Libraries) before other cells

### "RuntimeError: This event loop is already running"
â†’ Already fixed! TTS uses `nest_asyncio` for Jupyter compatibility

### "Audio generation error"
â†’ TTS may have network issues. Conversation continues in text mode.

### Models won't download
â†’ Check internet connection. Models cached after first download.

---

## ğŸ“ˆ Performance Metrics

| Model | Dataset | Accuracy | Speed |
|-------|---------|----------|-------|
| Vision (CNN) | KMNIST | ~95% | 10ms/inference |
| Audio (Wav2Vec2) | Japanese Speech | ~85% phoneme accuracy | 500ms (with preprocessing) |
| LLM Response | GPT-4o | N/A | 1-3 seconds |

---

## ğŸ“ Educational Design

### Closed-Loop Philosophy
1. **Student Input** â†’ Drawing + Speech
2. **Model Evaluation** â†’ Vision + Audio analysis
3. **Sensor Fusion** â†’ Combine results
4. **LLM Orchestration** â†’ Generate pedagogical response
5. **Adaptive Feedback** â†’ Specific, grounded in data

Unlike traditional tutoring apps:
- âŒ NOT multiple choice (forces production, not recognition)
- âœ… AI actually sees & hears mistakes
- âœ… Feedback is specific to student errors
- âœ… Dynamic lesson adaptation based on sensor data

---

## ğŸ”® Future Enhancements

- [ ] Stroke order verification for handwriting
- [ ] Real-time audio feedback (as user speaks)
- [ ] Spaced repetition scheduler
- [ ] Support for Katakana, Kanji, other languages
- [ ] Mobile app (React Native)
- [ ] Pronunciation video comparison
- [ ] Community lesson sharing

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see LICENSE file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“§ Support & Contact

- **Issues**: [GitHub Issues](https://github.com/YOUR_USERNAME/ai-sensei/issues)
- **Discussions**: [GitHub Discussions](https://github.com/YOUR_USERNAME/ai-sensei/discussions)
- **Email**: your-email@example.com

---

## ğŸ™ Acknowledgments

- KMNIST dataset for Hiragana training
- HuggingFace for Wav2Vec2 models
- OpenAI & Google for LLM APIs
- Jupyter community for notebook ecosystem

---

## ğŸ“± Social

- [Twitter](https://twitter.com/yourhandle)
- [LinkedIn](https://linkedin.com/in/yourprofile)

---

**Made with â¤ï¸ for Japanese language learners worldwide**

ğŸŒ **AI Sensei - Learn Japanese the way humans do** ğŸŒ
