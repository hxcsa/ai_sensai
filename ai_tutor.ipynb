{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631179dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“˜ AI Sensei: Multi-Modal Japanese Language Tutor\n",
    "\n",
    "**A closed-loop AI tutor that \"sees\" your handwriting and \"hears\" your pronunciation errors**\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This notebook implements a **Tri-Model System**:\n",
    "\n",
    "| Component | Model | Role |\n",
    "|-----------|-------|------|\n",
    "| **The Eyes** ğŸ‘ï¸ | Custom CNN (KMNIST) | Handwriting Recognition |\n",
    "| **The Ears** ğŸ‘‚ | Wav2Vec2 + CTC | Pronunciation Scoring |\n",
    "| **The Brain** ğŸ§  | GPT-4o / Gemini | Curriculum Orchestrator |\n",
    "\n",
    "The key innovation: Instead of simple pass/fail, we inject **raw sensor data** into the LLM prompt so it can give specific, grounded feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e394096",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies\n",
    "\n",
    "Install all required libraries for our three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425f1b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# @title Install Dependencies (Run this cell first!)\n",
    "# Core ML frameworks\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers --quiet\n",
    "\n",
    "# Audio processing\n",
    "!pip install librosa soundfile --quiet\n",
    "\n",
    "# Text processing for Japanese\n",
    "!pip install pykakasi python-Levenshtein --quiet\n",
    "\n",
    "# LLM API clients\n",
    "!pip install openai google-generativeai --quiet\n",
    "\n",
    "# Visualization & utilities\n",
    "!pip install matplotlib numpy pandas scikit-learn --quiet\n",
    "\n",
    "# Jupyter widgets for interactive UI\n",
    "!pip install ipywidgets ipycanvas --quiet\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383e1c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Using device: cuda\n",
      "   PyTorch version: 2.9.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# @title Import Libraries & Setup Device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1cc86",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Eyes ğŸ‘ï¸: Handwriting Recognition (Custom CNN)\n",
    "\n",
    "We implement a specialized **Convolutional Neural Network** trained on the **KMNIST** (Kuzushiji-MNIST) dataset to recognize handwritten Hiragana characters.\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input (28Ã—28) â†’ Conv2d(32) â†’ ReLU â†’ MaxPool \n",
    "             â†’ Conv2d(64) â†’ ReLU â†’ MaxPool \n",
    "             â†’ Flatten â†’ Dense(128) â†’ Output(49 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da335ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SenseiVisionNet initialized\n",
      "   Total parameters: 1,738,225\n"
     ]
    }
   ],
   "source": [
    "# @title Define CNN Architecture: SenseiVisionNet\n",
    "class SenseiVisionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN for Hiragana character recognition.\n",
    "    Trained on KMNIST dataset (49 Hiragana classes).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 49):\n",
    "        super(SenseiVisionNet, self).__init__()\n",
    "        \n",
    "        # Feature extraction blocks\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: Extract basic edges and strokes\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28x28 -> 14x14\n",
    "            \n",
    "            # Block 2: Extract complex patterns\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14x14 -> 7x7\n",
    "            \n",
    "            # Block 3: Higher-level features\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def predict_with_confidence(self, x: torch.Tensor) -> Tuple[int, float]:\n",
    "        \"\"\"Return predicted class and confidence score.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            confidence, pred_class = torch.max(probs, dim=1)\n",
    "        return pred_class.item(), confidence.item()\n",
    "\n",
    "# Initialize model\n",
    "model_vision = SenseiVisionNet().to(device)\n",
    "print(\"âœ… SenseiVisionNet initialized\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model_vision.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8a2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Character mapping loaded\n",
      "   Total characters: 49\n"
     ]
    }
   ],
   "source": [
    "# @title KMNIST Character Mapping\n",
    "# Official KMNIST class labels (Hiragana characters)\n",
    "KMNIST_LABELS = [\n",
    "    'ãŠ', 'ã', 'ã™', 'ã¤', 'ãª', 'ã¯', 'ã¾', 'ã‚„', 'ã‚Œ', 'ã‚’',  # 0-9\n",
    "    'ã‚', 'ã„', 'ã†', 'ãˆ', 'ã‹', 'ã‘', 'ã“', 'ã•', 'ã—', 'ã›',  # 10-19\n",
    "    'ã', 'ãŸ', 'ã¡', 'ã¦', 'ã¨', 'ã«', 'ã®', 'ã²', 'ãµ', 'ã¸',  # 20-29\n",
    "    'ã»', 'ã¿', 'ã‚€', 'ã‚', 'ã‚‚', 'ã‚†', 'ã‚ˆ', 'ã‚‰', 'ã‚Š', 'ã‚‹',  # 30-39\n",
    "    'ã‚', 'ã‚', 'ã‚“', 'ã¬', 'ã­', 'ã', 'ã', 'ã‚', 'ã‚‘'         # 40-48\n",
    "]\n",
    "\n",
    "# Romaji equivalents for feedback\n",
    "KMNIST_ROMAJI = [\n",
    "    'o', 'ki', 'su', 'tsu', 'na', 'ha', 'ma', 'ya', 're', 'wo',\n",
    "    'a', 'i', 'u', 'e', 'ka', 'ke', 'ko', 'sa', 'shi', 'se',\n",
    "    'so', 'ta', 'chi', 'te', 'to', 'ni', 'no', 'hi', 'fu', 'he',\n",
    "    'ho', 'mi', 'mu', 'me', 'mo', 'yu', 'yo', 'ra', 'ri', 'ru',\n",
    "    'ro', 'wa', 'n', 'nu', 'ne', 'ku', 'so', 'wi', 'we'\n",
    "]\n",
    "\n",
    "def get_character_info(class_idx: int) -> Dict[str, str]:\n",
    "    \"\"\"Get hiragana character and its romaji from class index.\"\"\"\n",
    "    if 0 <= class_idx < len(KMNIST_LABELS):\n",
    "        return {\n",
    "            'hiragana': KMNIST_LABELS[class_idx],\n",
    "            'romaji': KMNIST_ROMAJI[class_idx]\n",
    "        }\n",
    "    return {'hiragana': '?', 'romaji': 'unknown'}\n",
    "\n",
    "print(\"âœ… Character mapping loaded\")\n",
    "print(f\"   Total characters: {len(KMNIST_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6716f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Downloading KMNIST dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.2M/18.2M [00:11<00:00, 1.58MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.2M/18.2M [00:11<00:00, 1.58MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 318kB/s]\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.04M/3.04M [00:01<00:00, 2.26MB/s]\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.12k/5.12k [00:00<00:00, 21.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training samples: 60,000\n",
      "   Test samples: 10,000\n",
      "\n",
      "ğŸ‹ï¸ Training 'The Eyes' for 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch [1/3], Step [200/938], Loss: 0.7292\n",
      "   Epoch [1/3], Step [400/938], Loss: 0.2717\n",
      "   Epoch [1/3], Step [400/938], Loss: 0.2717\n",
      "   Epoch [1/3], Step [600/938], Loss: 0.2041\n",
      "   Epoch [1/3], Step [600/938], Loss: 0.2041\n",
      "   Epoch [1/3], Step [800/938], Loss: 0.1694\n",
      "   Epoch [1/3], Step [800/938], Loss: 0.1694\n",
      "   âœ“ Epoch 1: Train Acc: 90.40%, Val Acc: 93.03%\n",
      "   âœ“ Epoch 1: Train Acc: 90.40%, Val Acc: 93.03%\n",
      "   Epoch [2/3], Step [200/938], Loss: 0.1211\n",
      "   Epoch [2/3], Step [200/938], Loss: 0.1211\n",
      "   Epoch [2/3], Step [400/938], Loss: 0.1257\n",
      "   Epoch [2/3], Step [400/938], Loss: 0.1257\n",
      "   Epoch [2/3], Step [600/938], Loss: 0.1151\n",
      "   Epoch [2/3], Step [600/938], Loss: 0.1151\n",
      "   Epoch [2/3], Step [800/938], Loss: 0.1053\n",
      "   Epoch [2/3], Step [800/938], Loss: 0.1053\n",
      "   âœ“ Epoch 2: Train Acc: 96.67%, Val Acc: 94.83%\n",
      "   âœ“ Epoch 2: Train Acc: 96.67%, Val Acc: 94.83%\n",
      "   Epoch [3/3], Step [200/938], Loss: 0.0723\n",
      "   Epoch [3/3], Step [200/938], Loss: 0.0723\n",
      "   Epoch [3/3], Step [400/938], Loss: 0.0631\n",
      "   Epoch [3/3], Step [400/938], Loss: 0.0631\n",
      "   Epoch [3/3], Step [600/938], Loss: 0.0557\n",
      "   Epoch [3/3], Step [600/938], Loss: 0.0557\n",
      "   Epoch [3/3], Step [800/938], Loss: 0.0580\n",
      "   Epoch [3/3], Step [800/938], Loss: 0.0580\n",
      "   âœ“ Epoch 3: Train Acc: 98.17%, Val Acc: 95.66%\n",
      "\n",
      "âœ… Training complete! Final accuracy: 95.66%\n",
      "   âœ“ Epoch 3: Train Acc: 98.17%, Val Acc: 95.66%\n",
      "\n",
      "âœ… Training complete! Final accuracy: 95.66%\n"
     ]
    }
   ],
   "source": [
    "# @title Train Vision Model on KMNIST\n",
    "def train_vision_model(model: nn.Module, epochs: int = 5, batch_size: int = 64) -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Train the CNN on KMNIST dataset.\n",
    "    Returns training history for visualization.\n",
    "    \"\"\"\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # Load KMNIST dataset\n",
    "    print(\"ğŸ“¥ Downloading KMNIST dataset...\")\n",
    "    trainset = torchvision.datasets.KMNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    testset = torchvision.datasets.KMNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"   Training samples: {len(trainset):,}\")\n",
    "    print(f\"   Test samples: {len(testset):,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"\\nğŸ‹ï¸ Training 'The Eyes' for {epochs} epochs...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if (i + 1) % 200 == 0:\n",
    "                print(f'   Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(trainloader)}], '\n",
    "                      f'Loss: {running_loss/200:.4f}')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        train_acc = 100 * correct / total\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'   âœ“ Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "    \n",
    "    print(f\"\\nâœ… Training complete! Final accuracy: {val_acc:.2f}%\")\n",
    "    return history\n",
    "\n",
    "# Train the model (set epochs=1 for quick demo, increase for better results)\n",
    "history = train_vision_model(model_vision, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c5508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vision inference helper ready\n"
     ]
    }
   ],
   "source": [
    "# @title Vision Model Inference Helper\n",
    "def analyze_handwriting(image_tensor: torch.Tensor) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a handwritten character image.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Preprocessed 28x28 tensor\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction, confidence, and feedback\n",
    "    \"\"\"\n",
    "    model_vision.eval()\n",
    "    \n",
    "    # Ensure correct shape: [1, 1, 28, 28]\n",
    "    if image_tensor.dim() == 2:\n",
    "        image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    elif image_tensor.dim() == 3:\n",
    "        image_tensor = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_vision(image_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        confidence, predicted_class = torch.max(probs, dim=1)\n",
    "        \n",
    "        # Get top 3 predictions for feedback\n",
    "        top3_probs, top3_classes = torch.topk(probs, 3, dim=1)\n",
    "    \n",
    "    char_info = get_character_info(predicted_class.item())\n",
    "    \n",
    "    # Calculate score (0-100 based on confidence)\n",
    "    score = int(confidence.item() * 100)\n",
    "    \n",
    "    # Generate feedback\n",
    "    if score >= 80:\n",
    "        feedback = f\"Excellent! Clear '{char_info['hiragana']}' ({char_info['romaji']})\"\n",
    "    elif score >= 60:\n",
    "        feedback = f\"Good attempt at '{char_info['hiragana']}'. Work on stroke clarity.\"\n",
    "    else:\n",
    "        alternatives = [get_character_info(c.item())['hiragana'] for c in top3_classes[0][1:]]\n",
    "        feedback = f\"Unclear. Could be '{char_info['hiragana']}' or {alternatives}. Practice stroke order.\"\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': predicted_class.item(),\n",
    "        'predicted_char': char_info['hiragana'],\n",
    "        'predicted_romaji': char_info['romaji'],\n",
    "        'confidence': confidence.item(),\n",
    "        'score': score,\n",
    "        'feedback': feedback,\n",
    "        'top3': [(get_character_info(c.item()), p.item()) \n",
    "                 for c, p in zip(top3_classes[0], top3_probs[0])]\n",
    "    }\n",
    "\n",
    "print(\"âœ… Vision inference helper ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b9a7c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. The Ears ğŸ‘‚: Pronunciation Scoring (Wav2Vec2 + CTC)\n",
    "\n",
    "We use **Wav2Vec2** for speech recognition and **Levenshtein distance** for phoneme-level error detection.\n",
    "\n",
    "### Why CTC (Connectionist Temporal Classification)?\n",
    "- People speak at different speeds\n",
    "- CTC aligns audio frames to text without explicit timing\n",
    "- We can pinpoint **which specific sounds** were mispronounced\n",
    "\n",
    "### Scoring Pipeline:\n",
    "```\n",
    "Audio â†’ Wav2Vec2 â†’ Transcription â†’ Romaji Conversion â†’ Levenshtein Distance â†’ Score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3191319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading 'The Ears' (Wav2Vec2)...\n",
      "   This may take a minute on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3318ac3086dc4842aa3cfe20bb22a9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6df90906724eb0aee96fedbd0b52cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45961e128dea471ba9e47b6d0469051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad10a5a1f5c4cb79bedb82920ad243f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b7e43e4a24403da8d68038e1c7321f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc981df64c07475eb907960839e0176d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wav2Vec2 Model Loaded Successfully\n",
      "   Model: jonatasgrosman/wav2vec2-large-xlsr-53-japanese\n"
     ]
    }
   ],
   "source": [
    "# @title Load Wav2Vec2 Model for Japanese\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import librosa\n",
    "\n",
    "# Pre-trained Japanese speech recognition model\n",
    "AUDIO_MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\"\n",
    "\n",
    "print(\"ğŸ“¥ Loading 'The Ears' (Wav2Vec2)...\")\n",
    "print(\"   This may take a minute on first run...\")\n",
    "\n",
    "try:\n",
    "    processor_audio = Wav2Vec2Processor.from_pretrained(AUDIO_MODEL_ID)\n",
    "    model_audio = Wav2Vec2ForCTC.from_pretrained(AUDIO_MODEL_ID).to(device)\n",
    "    print(\"âœ… Wav2Vec2 Model Loaded Successfully\")\n",
    "    print(f\"   Model: {AUDIO_MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not load model: {e}\")\n",
    "    print(\"   Will use mock audio scoring for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fd8a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text processing ready\n",
      "   Test: 'ã“ã‚“ã«ã¡ã¯' â†’ 'konnichiha'\n",
      "   Phonemes: ['ko', 'n', 'n', 'i', 'chi', 'ha']\n"
     ]
    }
   ],
   "source": [
    "# @title Japanese Text Processing Utilities\n",
    "from pykakasi import kakasi\n",
    "import Levenshtein\n",
    "\n",
    "# Initialize Kakasi for Japanese -> Romaji conversion\n",
    "kks = kakasi()\n",
    "kks.setMode(\"H\", \"a\")  # Hiragana to ascii\n",
    "kks.setMode(\"K\", \"a\")  # Katakana to ascii  \n",
    "kks.setMode(\"J\", \"a\")  # Japanese to ascii\n",
    "converter = kks.getConverter()\n",
    "\n",
    "def to_romaji(text: str) -> str:\n",
    "    \"\"\"Convert Japanese text to romaji for phoneme comparison.\"\"\"\n",
    "    return converter.do(text).lower().replace(\" \", \"\")\n",
    "\n",
    "def to_phonemes(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Break Japanese text into phoneme components.\n",
    "    Example: 'ã“ã‚“ã«ã¡ã¯' -> ['ko', 'n', 'ni', 'chi', 'wa']\n",
    "    \"\"\"\n",
    "    romaji = to_romaji(text)\n",
    "    # Simple phoneme splitting (can be enhanced)\n",
    "    phonemes = []\n",
    "    i = 0\n",
    "    while i < len(romaji):\n",
    "        # Check for two-character phonemes\n",
    "        if i + 1 < len(romaji) and romaji[i:i+2] in ['ch', 'sh', 'ts']:\n",
    "            if i + 2 < len(romaji) and romaji[i+2] in 'aiueo':\n",
    "                phonemes.append(romaji[i:i+3])\n",
    "                i += 3\n",
    "                continue\n",
    "        # Check for consonant + vowel\n",
    "        if i + 1 < len(romaji) and romaji[i] not in 'aiueon' and romaji[i+1] in 'aiueo':\n",
    "            phonemes.append(romaji[i:i+2])\n",
    "            i += 2\n",
    "        else:\n",
    "            phonemes.append(romaji[i])\n",
    "            i += 1\n",
    "    return phonemes\n",
    "\n",
    "# Test the converter\n",
    "test_phrase = \"ã“ã‚“ã«ã¡ã¯\"\n",
    "print(f\"âœ… Text processing ready\")\n",
    "print(f\"   Test: '{test_phrase}' â†’ '{to_romaji(test_phrase)}'\")\n",
    "print(f\"   Phonemes: {to_phonemes(test_phrase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1aea072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pronunciation scoring function ready\n"
     ]
    }
   ],
   "source": [
    "# @title Pronunciation Scoring Function\n",
    "def calculate_pronunciation_score(\n",
    "    target_text: str, \n",
    "    audio_array: np.ndarray,\n",
    "    sample_rate: int = 16000\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Score pronunciation using Wav2Vec2 + Levenshtein distance.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Transcribe audio using Wav2Vec2\n",
    "    2. Convert both target and actual to romaji (phoneme approximation)\n",
    "    3. Calculate Levenshtein distance for error detection\n",
    "    4. Return detailed scoring with specific error indices\n",
    "    \n",
    "    Args:\n",
    "        target_text: Expected Japanese text\n",
    "        audio_array: Audio waveform as numpy array\n",
    "        sample_rate: Audio sample rate (default 16kHz for Wav2Vec2)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with score, transcription, and error details\n",
    "    \"\"\"\n",
    "    # Resample to 16kHz if needed (Wav2Vec2 requirement)\n",
    "    if sample_rate != 16000:\n",
    "        audio_array = librosa.resample(\n",
    "            audio_array.astype(np.float32), \n",
    "            orig_sr=sample_rate, \n",
    "            target_sr=16000\n",
    "        )\n",
    "    \n",
    "    # Ensure float32 and normalize\n",
    "    audio_array = audio_array.astype(np.float32)\n",
    "    if np.max(np.abs(audio_array)) > 1.0:\n",
    "        audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "    \n",
    "    # 1. Transcribe with Wav2Vec2\n",
    "    try:\n",
    "        inputs = processor_audio(\n",
    "            audio_array, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        model_audio.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model_audio(inputs.input_values.to(device)).logits\n",
    "        \n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor_audio.batch_decode(predicted_ids)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Transcription error: {e}\")\n",
    "        transcription = \"\"\n",
    "    \n",
    "    # 2. Convert to romaji for comparison\n",
    "    target_romaji = to_romaji(target_text)\n",
    "    actual_romaji = to_romaji(transcription) if transcription else \"\"\n",
    "    \n",
    "    target_phonemes = to_phonemes(target_text)\n",
    "    actual_phonemes = to_phonemes(transcription) if transcription else []\n",
    "    \n",
    "    # 3. Calculate Levenshtein distance\n",
    "    if not actual_romaji:\n",
    "        distance = len(target_romaji)\n",
    "        score = 0\n",
    "    else:\n",
    "        distance = Levenshtein.distance(target_romaji, actual_romaji)\n",
    "        max_len = max(len(target_romaji), len(actual_romaji))\n",
    "        score = max(0, int((1 - distance / max_len) * 100)) if max_len > 0 else 0\n",
    "    \n",
    "    # 4. Find specific error positions using edit operations\n",
    "    edit_ops = Levenshtein.editops(target_romaji, actual_romaji) if actual_romaji else []\n",
    "    \n",
    "    error_indices = []\n",
    "    error_types = []\n",
    "    for op, src_pos, dest_pos in edit_ops:\n",
    "        error_indices.append(src_pos)\n",
    "        error_types.append(op)  # 'replace', 'insert', 'delete'\n",
    "    \n",
    "    # Generate feedback\n",
    "    if score >= 90:\n",
    "        feedback = \"Excellent pronunciation! å®Œç’§ã§ã™ï¼\"\n",
    "    elif score >= 70:\n",
    "        feedback = f\"Good! Minor issues at positions: {error_indices[:3]}\"\n",
    "    elif score >= 50:\n",
    "        feedback = f\"Keep practicing. You said '{actual_romaji}' instead of '{target_romaji}'\"\n",
    "    else:\n",
    "        feedback = f\"Let's slow down. Target: '{target_romaji}'. Heard: '{actual_romaji or 'nothing'}'\"\n",
    "    \n",
    "    return {\n",
    "        'score': score,\n",
    "        'transcription': transcription,\n",
    "        'target_text': target_text,\n",
    "        'target_romaji': target_romaji,\n",
    "        'actual_romaji': actual_romaji,\n",
    "        'target_phonemes': target_phonemes,\n",
    "        'actual_phonemes': actual_phonemes,\n",
    "        'edit_distance': distance,\n",
    "        'error_indices': error_indices,\n",
    "        'error_types': error_types,\n",
    "        'feedback': feedback\n",
    "    }\n",
    "\n",
    "print(\"âœ… Pronunciation scoring function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdc09c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The Brain ğŸ§ : Curriculum Orchestrator (LLM)\n",
    "\n",
    "The Brain isn't just a chatbotâ€”it's a **dynamic tutor** that uses raw sensor data to provide specific, grounded feedback.\n",
    "\n",
    "### Dynamic Prompt Injection\n",
    "Instead of hardcoding responses, we inject the vision/audio scores directly into the system prompt:\n",
    "\n",
    "```python\n",
    "Input Data: {audio_score: 45, error_type: \"phoneme_mismatch\", detected: \"Konowa\"}\n",
    "Instruction: \"The student skipped the middle syllables. Explain that 'ni-chi' is missing.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "792c851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM Configuration\n",
      "   Provider: openai\n",
      "   Model: gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "# @title Configure LLM API\n",
    "import os\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”‘ SET YOUR API KEY HERE\n",
    "# Choose ONE of the following options:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Option 1: OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n",
    "\n",
    "# Option 2: Google Gemini (alternative)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY_HERE\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Configuration\n",
    "LLM_PROVIDER = \"openai\"  # Change to \"gemini\" if using Google\n",
    "LLM_MODEL = \"gpt-5-mini\"     # Or \"gemini-1.5-pro\"\n",
    "\n",
    "print(f\"âœ… LLM Configuration\")\n",
    "print(f\"   Provider: {LLM_PROVIDER}\")\n",
    "print(f\"   Model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c25ab3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM Orchestrator ready\n"
     ]
    }
   ],
   "source": [
    "# @title LLM Orchestrator: The Brain\n",
    "def get_tutor_response(\n",
    "    student_profile: Dict[str, Any],\n",
    "    current_phrase: str,\n",
    "    vision_result: Optional[Dict[str, Any]] = None,\n",
    "    audio_result: Optional[Dict[str, Any]] = None,\n",
    "    lesson_history: Optional[list] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate pedagogical response by injecting sensor data into the LLM prompt.\n",
    "    \n",
    "    This is the core \"closed loop\" - the LLM sees exactly what the models detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build sensor data summary\n",
    "    vision_score = vision_result.get('score', 0) if vision_result else None\n",
    "    audio_score = audio_result.get('score', 0) if audio_result else None\n",
    "    \n",
    "    vision_feedback = vision_result.get('feedback', 'No handwriting submitted') if vision_result else 'No handwriting submitted'\n",
    "    audio_feedback = audio_result.get('feedback', 'No audio submitted') if audio_result else 'No audio submitted'\n",
    "    \n",
    "    # Construct the dynamic system prompt with sensor data injection\n",
    "    system_prompt = f\"\"\"You are AI Sensei (AIå…ˆç”Ÿ), a patient but rigorous Japanese language tutor.\n",
    "\n",
    "STUDENT PROFILE:\n",
    "- Name: {student_profile.get('name', 'Student')}\n",
    "- Level: {student_profile.get('level', 'Beginner')}\n",
    "- Focus: {student_profile.get('focus', 'Hiragana basics')}\n",
    "\n",
    "CURRENT LESSON:\n",
    "- Target Phrase: {current_phrase}\n",
    "- Target Romaji: {to_romaji(current_phrase)}\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SENSOR DATA FROM PERCEPTION MODELS (Use this for feedback!)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“ HANDWRITING ANALYSIS (The Eyes):\n",
    "- Score: {vision_score}/100\n",
    "- Detection: {vision_feedback}\n",
    "{f\"- Predicted Character: {vision_result.get('predicted_char', 'N/A')}\" if vision_result else \"\"}\n",
    "\n",
    "ğŸ¤ PRONUNCIATION ANALYSIS (The Ears):\n",
    "- Score: {audio_score}/100  \n",
    "- Target Phonemes: {audio_result.get('target_romaji', 'N/A') if audio_result else 'N/A'}\n",
    "- Heard Phonemes: {audio_result.get('actual_romaji', 'N/A') if audio_result else 'N/A'}\n",
    "- Error Positions: {audio_result.get('error_indices', []) if audio_result else []}\n",
    "- Feedback: {audio_feedback}\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Analyze the sensor scores above (these are REAL measurements, not guesses)\n",
    "2. If pronunciation score < 60: Explain SPECIFICALLY which sounds were wrong based on error positions\n",
    "3. If handwriting score < 60: Give specific stroke order advice\n",
    "4. If both scores > 80: Celebrate and introduce the next challenge\n",
    "5. Be encouraging but honest - the data doesn't lie!\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Keep response under 100 words\n",
    "- Use some Japanese phrases with translations\n",
    "- End with a specific next action for the student\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response using configured LLM\n",
    "    if LLM_PROVIDER == \"openai\":\n",
    "        try:\n",
    "            import openai\n",
    "            client = openai.OpenAI()\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Please evaluate my attempt at '{current_phrase}'\"}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"[LLM Error: {e}]\\n\\nBased on sensor data:\\n- Handwriting: {vision_score}/100\\n- Pronunciation: {audio_score}/100\\n{audio_feedback}\"\n",
    "    \n",
    "    elif LLM_PROVIDER == \"gemini\":\n",
    "        try:\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "            \n",
    "            model = genai.GenerativeModel(LLM_MODEL)\n",
    "            response = model.generate_content(system_prompt)\n",
    "            return response.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"[LLM Error: {e}]\\n\\nBased on sensor data:\\n- Handwriting: {vision_score}/100\\n- Pronunciation: {audio_score}/100\"\n",
    "    \n",
    "    else:\n",
    "        # Fallback: Generate response without LLM\n",
    "        return generate_fallback_response(vision_result, audio_result, current_phrase)\n",
    "\n",
    "\n",
    "def generate_fallback_response(vision_result, audio_result, phrase):\n",
    "    \"\"\"Generate feedback without LLM API (for testing/offline use).\"\"\"\n",
    "    v_score = vision_result.get('score', 0) if vision_result else 0\n",
    "    a_score = audio_result.get('score', 0) if audio_result else 0\n",
    "    \n",
    "    response = f\"ğŸ“Š Results for '{phrase}':\\n\\n\"\n",
    "    \n",
    "    if v_score >= 80 and a_score >= 80:\n",
    "        response += \"ğŸŒŸ ç´ æ™´ã‚‰ã—ã„ (Subarashii)! Excellent work on both writing and pronunciation!\\n\"\n",
    "    elif v_score < 60:\n",
    "        response += f\"âœï¸ Your handwriting needs practice. {vision_result.get('feedback', '')}\\n\"\n",
    "    elif a_score < 60:\n",
    "        response += f\"ğŸ¤ Let's work on pronunciation. {audio_result.get('feedback', '')}\\n\"\n",
    "    else:\n",
    "        response += \"ğŸ‘ Good progress! Keep practicing.\\n\"\n",
    "    \n",
    "    response += f\"\\nğŸ“ˆ Scores: Writing {v_score}/100 | Speaking {a_score}/100\"\n",
    "    return response\n",
    "\n",
    "print(\"âœ… LLM Orchestrator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b8f69",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. User Interface: Input Widgets\n",
    "\n",
    "Interactive components for capturing handwriting (canvas) and speech (microphone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89b11822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Drawing canvas widget ready\n"
     ]
    }
   ],
   "source": [
    "# @title Drawing Canvas Widget (HTML/JS)\n",
    "from IPython.display import display, HTML, Javascript, clear_output\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Global state for captured inputs\n",
    "captured_inputs = {\n",
    "    'image_data': None,\n",
    "    'audio_data': None\n",
    "}\n",
    "\n",
    "def create_drawing_canvas(target_char: str = \"ã‚\") -> HTML:\n",
    "    \"\"\"Create an interactive drawing canvas for handwriting input.\"\"\"\n",
    "    \n",
    "    canvas_html = f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; padding: 10px;\">\n",
    "        <h3>âœï¸ Write: <span style=\"font-size: 48px; color: #e74c3c;\">{target_char}</span></h3>\n",
    "        <canvas id=\"drawingCanvas\" width=\"280\" height=\"280\" \n",
    "                style=\"border: 3px solid #333; background: white; cursor: crosshair; border-radius: 8px;\"></canvas>\n",
    "        <br><br>\n",
    "        <button onclick=\"clearCanvas()\" style=\"padding: 10px 20px; margin: 5px; cursor: pointer; \n",
    "                background: #e74c3c; color: white; border: none; border-radius: 5px;\">ğŸ—‘ï¸ Clear</button>\n",
    "        <button onclick=\"submitDrawing()\" style=\"padding: 10px 20px; margin: 5px; cursor: pointer;\n",
    "                background: #27ae60; color: white; border: none; border-radius: 5px;\">âœ… Submit</button>\n",
    "        <p id=\"canvasStatus\" style=\"color: #666;\">Draw the character above, then click Submit.</p>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "    (function() {{\n",
    "        var canvas = document.getElementById('drawingCanvas');\n",
    "        var ctx = canvas.getContext('2d');\n",
    "        var drawing = false;\n",
    "        var lastX = 0, lastY = 0;\n",
    "        \n",
    "        // Set drawing style\n",
    "        ctx.strokeStyle = '#000';\n",
    "        ctx.lineWidth = 8;\n",
    "        ctx.lineCap = 'round';\n",
    "        ctx.lineJoin = 'round';\n",
    "        \n",
    "        // Fill white background\n",
    "        ctx.fillStyle = 'white';\n",
    "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
    "        \n",
    "        canvas.addEventListener('mousedown', function(e) {{\n",
    "            drawing = true;\n",
    "            lastX = e.offsetX;\n",
    "            lastY = e.offsetY;\n",
    "        }});\n",
    "        \n",
    "        canvas.addEventListener('mousemove', function(e) {{\n",
    "            if (!drawing) return;\n",
    "            ctx.beginPath();\n",
    "            ctx.moveTo(lastX, lastY);\n",
    "            ctx.lineTo(e.offsetX, e.offsetY);\n",
    "            ctx.stroke();\n",
    "            lastX = e.offsetX;\n",
    "            lastY = e.offsetY;\n",
    "        }});\n",
    "        \n",
    "        canvas.addEventListener('mouseup', function() {{ drawing = false; }});\n",
    "        canvas.addEventListener('mouseout', function() {{ drawing = false; }});\n",
    "        \n",
    "        // Touch support\n",
    "        canvas.addEventListener('touchstart', function(e) {{\n",
    "            e.preventDefault();\n",
    "            var touch = e.touches[0];\n",
    "            var rect = canvas.getBoundingClientRect();\n",
    "            drawing = true;\n",
    "            lastX = touch.clientX - rect.left;\n",
    "            lastY = touch.clientY - rect.top;\n",
    "        }});\n",
    "        \n",
    "        canvas.addEventListener('touchmove', function(e) {{\n",
    "            e.preventDefault();\n",
    "            if (!drawing) return;\n",
    "            var touch = e.touches[0];\n",
    "            var rect = canvas.getBoundingClientRect();\n",
    "            ctx.beginPath();\n",
    "            ctx.moveTo(lastX, lastY);\n",
    "            ctx.lineTo(touch.clientX - rect.left, touch.clientY - rect.top);\n",
    "            ctx.stroke();\n",
    "            lastX = touch.clientX - rect.left;\n",
    "            lastY = touch.clientY - rect.top;\n",
    "        }});\n",
    "        \n",
    "        canvas.addEventListener('touchend', function() {{ drawing = false; }});\n",
    "        \n",
    "        window.clearCanvas = function() {{\n",
    "            ctx.fillStyle = 'white';\n",
    "            ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
    "            document.getElementById('canvasStatus').innerText = 'Canvas cleared. Draw again!';\n",
    "        }};\n",
    "        \n",
    "        window.submitDrawing = function() {{\n",
    "            var dataURL = canvas.toDataURL('image/png');\n",
    "            document.getElementById('canvasStatus').innerText = 'âœ… Drawing captured!';\n",
    "            \n",
    "            // Send to Python kernel (works in Colab/Jupyter)\n",
    "            if (typeof google !== 'undefined' && google.colab) {{\n",
    "                google.colab.kernel.invokeFunction('notebook.capture_image', [dataURL], {{}});\n",
    "            }} else {{\n",
    "                // For standard Jupyter, store in a global variable\n",
    "                window.capturedImageData = dataURL;\n",
    "                console.log('Image captured (length: ' + dataURL.length + ')');\n",
    "            }}\n",
    "        }};\n",
    "    }})();\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(canvas_html)\n",
    "\n",
    "print(\"âœ… Drawing canvas widget ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c13aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio recorder widget ready\n"
     ]
    }
   ],
   "source": [
    "# @title Audio Recorder Widget (HTML/JS)\n",
    "def create_audio_recorder(target_phrase: str = \"ã“ã‚“ã«ã¡ã¯\") -> HTML:\n",
    "    \"\"\"Create an interactive audio recorder for pronunciation input.\"\"\"\n",
    "    \n",
    "    audio_html = f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; padding: 10px;\">\n",
    "        <h3>ğŸ¤ Say: <span style=\"font-size: 32px; color: #3498db;\">{target_phrase}</span></h3>\n",
    "        <p style=\"color: #666;\">({to_romaji(target_phrase)})</p>\n",
    "        \n",
    "        <button id=\"recordBtn\" onclick=\"toggleRecording()\" \n",
    "                style=\"padding: 15px 30px; margin: 10px; cursor: pointer; font-size: 18px;\n",
    "                       background: #3498db; color: white; border: none; border-radius: 8px;\">\n",
    "            ğŸ™ï¸ Start Recording\n",
    "        </button>\n",
    "        \n",
    "        <div id=\"audioStatus\" style=\"margin: 10px; padding: 10px; background: #f0f0f0; border-radius: 5px;\">\n",
    "            Click the button to start recording your pronunciation.\n",
    "        </div>\n",
    "        \n",
    "        <audio id=\"audioPlayback\" controls style=\"display: none; margin-top: 10px;\"></audio>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "    (function() {{\n",
    "        let mediaRecorder;\n",
    "        let audioChunks = [];\n",
    "        let isRecording = false;\n",
    "        \n",
    "        window.toggleRecording = async function() {{\n",
    "            const btn = document.getElementById('recordBtn');\n",
    "            const status = document.getElementById('audioStatus');\n",
    "            \n",
    "            if (!isRecording) {{\n",
    "                // Start recording\n",
    "                try {{\n",
    "                    const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }});\n",
    "                    mediaRecorder = new MediaRecorder(stream);\n",
    "                    audioChunks = [];\n",
    "                    \n",
    "                    mediaRecorder.ondataavailable = event => audioChunks.push(event.data);\n",
    "                    \n",
    "                    mediaRecorder.onstop = async () => {{\n",
    "                        const audioBlob = new Blob(audioChunks, {{ type: 'audio/wav' }});\n",
    "                        const audioUrl = URL.createObjectURL(audioBlob);\n",
    "                        \n",
    "                        // Show playback\n",
    "                        const playback = document.getElementById('audioPlayback');\n",
    "                        playback.src = audioUrl;\n",
    "                        playback.style.display = 'block';\n",
    "                        \n",
    "                        // Convert to base64 for Python\n",
    "                        const reader = new FileReader();\n",
    "                        reader.readAsDataURL(audioBlob);\n",
    "                        reader.onloadend = () => {{\n",
    "                            const base64Audio = reader.result;\n",
    "                            status.innerHTML = 'âœ… Audio captured! (' + Math.round(audioBlob.size/1024) + ' KB)';\n",
    "                            \n",
    "                            // Send to Python kernel\n",
    "                            if (typeof google !== 'undefined' && google.colab) {{\n",
    "                                google.colab.kernel.invokeFunction('notebook.capture_audio', [base64Audio], {{}});\n",
    "                            }} else {{\n",
    "                                window.capturedAudioData = base64Audio;\n",
    "                                console.log('Audio captured (size: ' + audioBlob.size + ')');\n",
    "                            }}\n",
    "                        }};\n",
    "                        \n",
    "                        // Stop all tracks\n",
    "                        stream.getTracks().forEach(track => track.stop());\n",
    "                    }};\n",
    "                    \n",
    "                    mediaRecorder.start();\n",
    "                    isRecording = true;\n",
    "                    btn.innerHTML = 'â¹ï¸ Stop Recording';\n",
    "                    btn.style.background = '#e74c3c';\n",
    "                    status.innerHTML = 'ğŸ”´ Recording... Speak now!';\n",
    "                    \n",
    "                }} catch (err) {{\n",
    "                    status.innerHTML = 'âŒ Microphone access denied: ' + err.message;\n",
    "                }}\n",
    "            }} else {{\n",
    "                // Stop recording\n",
    "                mediaRecorder.stop();\n",
    "                isRecording = false;\n",
    "                btn.innerHTML = 'ğŸ™ï¸ Start Recording';\n",
    "                btn.style.background = '#3498db';\n",
    "                status.innerHTML = 'â³ Processing audio...';\n",
    "            }}\n",
    "        }};\n",
    "    }})();\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(audio_html)\n",
    "\n",
    "print(\"âœ… Audio recorder widget ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e49ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Image and audio processing utilities ready\n"
     ]
    }
   ],
   "source": [
    "# @title Image Processing Utilities\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def process_canvas_image(base64_data: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Process base64 canvas image into a tensor for the CNN.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Decode base64 to image\n",
    "    2. Convert to grayscale\n",
    "    3. Resize to 28x28\n",
    "    4. Invert colors (KMNIST uses white-on-black)\n",
    "    5. Normalize and convert to tensor\n",
    "    \"\"\"\n",
    "    # Decode base64\n",
    "    if ',' in base64_data:\n",
    "        base64_data = base64_data.split(',')[1]\n",
    "    \n",
    "    image_bytes = base64.b64decode(base64_data)\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    \n",
    "    # Convert to grayscale numpy array\n",
    "    image_gray = image.convert('L')\n",
    "    image_array = np.array(image_gray)\n",
    "    \n",
    "    # Resize to 28x28\n",
    "    image_resized = cv2.resize(image_array, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Invert colors (canvas is black-on-white, KMNIST is white-on-black)\n",
    "    image_inverted = 255 - image_resized\n",
    "    \n",
    "    # Normalize to [-1, 1] (same as training)\n",
    "    image_normalized = (image_inverted / 255.0 - 0.5) / 0.5\n",
    "    \n",
    "    # Convert to tensor\n",
    "    tensor = torch.tensor(image_normalized, dtype=torch.float32)\n",
    "    tensor = tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def process_audio_data(base64_data: str) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Process base64 audio data into numpy array for Wav2Vec2.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (audio_array, sample_rate)\n",
    "    \"\"\"\n",
    "    import soundfile as sf\n",
    "    \n",
    "    # Decode base64\n",
    "    if ',' in base64_data:\n",
    "        base64_data = base64_data.split(',')[1]\n",
    "    \n",
    "    audio_bytes = base64.b64decode(base64_data)\n",
    "    audio_io = BytesIO(audio_bytes)\n",
    "    \n",
    "    try:\n",
    "        audio_array, sample_rate = sf.read(audio_io)\n",
    "        \n",
    "        # Convert stereo to mono if needed\n",
    "        if len(audio_array.shape) > 1:\n",
    "            audio_array = np.mean(audio_array, axis=1)\n",
    "        \n",
    "        return audio_array, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Audio processing error: {e}\")\n",
    "        return np.zeros(16000), 16000  # Return silence on error\n",
    "\n",
    "print(\"âœ… Image and audio processing utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041a19f",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ğŸš€ The Integrated Lesson Loop\n",
    "\n",
    "This is where everything comes together: The **closed feedback loop** that makes AI Sensei special.\n",
    "\n",
    "```\n",
    "Student Input â†’ [The Eyes / The Ears] â†’ Sensor Data â†’ [The Brain] â†’ Personalized Feedback\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef853dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Curriculum loaded\n",
      "   Beginner lessons: 8\n",
      "   Intermediate lessons: 5\n"
     ]
    }
   ],
   "source": [
    "# @title Curriculum Configuration\n",
    "# Define the lesson curriculum\n",
    "CURRICULUM = {\n",
    "    'beginner': {\n",
    "        'name': 'Hiragana Basics',\n",
    "        'lessons': [\n",
    "            {'phrase': 'ã‚', 'romaji': 'a', 'meaning': 'Vowel A'},\n",
    "            {'phrase': 'ã„', 'romaji': 'i', 'meaning': 'Vowel I'},\n",
    "            {'phrase': 'ã†', 'romaji': 'u', 'meaning': 'Vowel U'},\n",
    "            {'phrase': 'ãˆ', 'romaji': 'e', 'meaning': 'Vowel E'},\n",
    "            {'phrase': 'ãŠ', 'romaji': 'o', 'meaning': 'Vowel O'},\n",
    "            {'phrase': 'ã‹', 'romaji': 'ka', 'meaning': 'K-row start'},\n",
    "            {'phrase': 'ã', 'romaji': 'ki', 'meaning': 'Tree (æœ¨)'},\n",
    "            {'phrase': 'ã“ã‚“ã«ã¡ã¯', 'romaji': 'konnichiwa', 'meaning': 'Hello'},\n",
    "        ]\n",
    "    },\n",
    "    'intermediate': {\n",
    "        'name': 'Common Phrases',\n",
    "        'lessons': [\n",
    "            {'phrase': 'ã‚ã‚ŠãŒã¨ã†', 'romaji': 'arigatou', 'meaning': 'Thank you'},\n",
    "            {'phrase': 'ã™ã¿ã¾ã›ã‚“', 'romaji': 'sumimasen', 'meaning': 'Excuse me'},\n",
    "            {'phrase': 'ãŠã¯ã‚ˆã†', 'romaji': 'ohayou', 'meaning': 'Good morning'},\n",
    "            {'phrase': 'ã•ã‚ˆã†ãªã‚‰', 'romaji': 'sayounara', 'meaning': 'Goodbye'},\n",
    "            {'phrase': 'ã¯ã˜ã‚ã¾ã—ã¦', 'romaji': 'hajimemashite', 'meaning': 'Nice to meet you'},\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Student profile (can be customized)\n",
    "student_profile = {\n",
    "    'name': 'Student',\n",
    "    'level': 'beginner',\n",
    "    'focus': 'Hiragana basics',\n",
    "    'completed_lessons': [],\n",
    "    'current_lesson_idx': 0\n",
    "}\n",
    "\n",
    "print(\"âœ… Curriculum loaded\")\n",
    "print(f\"   Beginner lessons: {len(CURRICULUM['beginner']['lessons'])}\")\n",
    "print(f\"   Intermediate lessons: {len(CURRICULUM['intermediate']['lessons'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de237104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AI Sensei Lesson Engine initialized\n"
     ]
    }
   ],
   "source": [
    "# @title AI Sensei Lesson Engine\n",
    "class AISenseiLesson:\n",
    "    \"\"\"\n",
    "    The complete lesson engine that orchestrates all three models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, student_profile: Dict[str, Any], curriculum: Dict):\n",
    "        self.student = student_profile\n",
    "        self.curriculum = curriculum\n",
    "        self.current_lesson = None\n",
    "        self.lesson_history = []\n",
    "        \n",
    "    def get_current_lesson(self) -> Dict:\n",
    "        \"\"\"Get the current lesson based on student progress.\"\"\"\n",
    "        level = self.student['level']\n",
    "        idx = self.student['current_lesson_idx']\n",
    "        lessons = self.curriculum[level]['lessons']\n",
    "        \n",
    "        if idx >= len(lessons):\n",
    "            # Level complete, move to next or wrap around\n",
    "            return lessons[-1]  # Repeat last lesson\n",
    "        \n",
    "        return lessons[idx]\n",
    "    \n",
    "    def display_lesson_ui(self):\n",
    "        \"\"\"Display the interactive lesson interface.\"\"\"\n",
    "        lesson = self.get_current_lesson()\n",
    "        self.current_lesson = lesson\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸ“š AI SENSEI - Lesson {self.student['current_lesson_idx'] + 1}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nğŸ¯ Target: {lesson['phrase']} ({lesson['romaji']})\")\n",
    "        print(f\"ğŸ“ Meaning: {lesson['meaning']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Display both input widgets\n",
    "        print(\"\\nğŸ“ STEP 1: Write the character\")\n",
    "        display(create_drawing_canvas(lesson['phrase']))\n",
    "        \n",
    "        print(\"\\nğŸ¤ STEP 2: Pronounce the phrase\")\n",
    "        display(create_audio_recorder(lesson['phrase']))\n",
    "        \n",
    "    def process_submission(self, image_data: str = None, audio_data: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process student submission through all three models.\n",
    "        This is the CLOSED LOOP.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'vision': None,\n",
    "            'audio': None,\n",
    "            'brain_response': None,\n",
    "            'overall_score': 0\n",
    "        }\n",
    "        \n",
    "        lesson = self.current_lesson or self.get_current_lesson()\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 1: The Eyes (Handwriting Analysis)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if image_data:\n",
    "            print(\"\\nğŸ‘ï¸ Analyzing handwriting...\")\n",
    "            try:\n",
    "                image_tensor = process_canvas_image(image_data)\n",
    "                results['vision'] = analyze_handwriting(image_tensor)\n",
    "                print(f\"   Score: {results['vision']['score']}/100\")\n",
    "                print(f\"   Detected: {results['vision']['predicted_char']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Vision error: {e}\")\n",
    "                results['vision'] = {'score': 0, 'feedback': 'Could not process image'}\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 2: The Ears (Pronunciation Analysis)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if audio_data:\n",
    "            print(\"\\nğŸ‘‚ Analyzing pronunciation...\")\n",
    "            try:\n",
    "                audio_array, sample_rate = process_audio_data(audio_data)\n",
    "                results['audio'] = calculate_pronunciation_score(\n",
    "                    lesson['phrase'], \n",
    "                    audio_array,\n",
    "                    sample_rate\n",
    "                )\n",
    "                print(f\"   Score: {results['audio']['score']}/100\")\n",
    "                print(f\"   Heard: {results['audio'].get('transcription', 'N/A')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Audio error: {e}\")\n",
    "                results['audio'] = {'score': 0, 'feedback': 'Could not process audio'}\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 3: The Brain (LLM Orchestration)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        print(\"\\nğŸ§  AI Sensei is thinking...\")\n",
    "        results['brain_response'] = get_tutor_response(\n",
    "            student_profile=self.student,\n",
    "            current_phrase=lesson['phrase'],\n",
    "            vision_result=results['vision'],\n",
    "            audio_result=results['audio'],\n",
    "            lesson_history=self.lesson_history\n",
    "        )\n",
    "        \n",
    "        # Calculate overall score\n",
    "        v_score = results['vision']['score'] if results['vision'] else 0\n",
    "        a_score = results['audio']['score'] if results['audio'] else 0\n",
    "        \n",
    "        if results['vision'] and results['audio']:\n",
    "            results['overall_score'] = (v_score + a_score) // 2\n",
    "        else:\n",
    "            results['overall_score'] = v_score or a_score\n",
    "        \n",
    "        # Record history\n",
    "        self.lesson_history.append({\n",
    "            'lesson': lesson,\n",
    "            'scores': {'vision': v_score, 'audio': a_score},\n",
    "            'overall': results['overall_score']\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, results: Dict[str, Any]):\n",
    "        \"\"\"Display the lesson results with AI Sensei's feedback.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š LESSON RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Score visualization\n",
    "        v_score = results['vision']['score'] if results['vision'] else 0\n",
    "        a_score = results['audio']['score'] if results['audio'] else 0\n",
    "        overall = results['overall_score']\n",
    "        \n",
    "        print(f\"\\nâœï¸ Handwriting: {'â–ˆ' * (v_score // 10)}{'â–‘' * (10 - v_score // 10)} {v_score}/100\")\n",
    "        print(f\"ğŸ¤ Pronunciation: {'â–ˆ' * (a_score // 10)}{'â–‘' * (10 - a_score // 10)} {a_score}/100\")\n",
    "        print(f\"ğŸ“ˆ Overall: {'â–ˆ' * (overall // 10)}{'â–‘' * (10 - overall // 10)} {overall}/100\")\n",
    "        \n",
    "        # Grade\n",
    "        if overall >= 90:\n",
    "            grade = \"ğŸŒŸ EXCELLENT!\"\n",
    "        elif overall >= 75:\n",
    "            grade = \"ğŸ‘ GOOD JOB!\"\n",
    "        elif overall >= 50:\n",
    "            grade = \"ğŸ“š KEEP PRACTICING\"\n",
    "        else:\n",
    "            grade = \"ğŸ’ª LET'S TRY AGAIN\"\n",
    "        \n",
    "        print(f\"\\n{grade}\")\n",
    "        \n",
    "        # AI Sensei's feedback\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"ğŸ¤– AI Sensei says:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(results['brain_response'])\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Progress check\n",
    "        if overall >= 75:\n",
    "            print(\"\\nâœ… Lesson passed! Moving to next lesson...\")\n",
    "            self.student['current_lesson_idx'] += 1\n",
    "        else:\n",
    "            print(\"\\nğŸ”„ Let's practice this one more time!\")\n",
    "    \n",
    "    def advance_lesson(self):\n",
    "        \"\"\"Move to the next lesson.\"\"\"\n",
    "        self.student['current_lesson_idx'] += 1\n",
    "        level = self.student['level']\n",
    "        if self.student['current_lesson_idx'] >= len(self.curriculum[level]['lessons']):\n",
    "            print(\"ğŸ‰ Congratulations! You've completed this level!\")\n",
    "            if level == 'beginner':\n",
    "                self.student['level'] = 'intermediate'\n",
    "                self.student['current_lesson_idx'] = 0\n",
    "                print(\"ğŸ“ˆ Advancing to Intermediate level!\")\n",
    "\n",
    "# Initialize the lesson engine\n",
    "sensei = AISenseiLesson(student_profile, CURRICULUM)\n",
    "print(\"âœ… AI Sensei Lesson Engine initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b916b825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘           ğŸŒ AI SENSEI - Japanese Tutor ğŸŒ                 â•‘\n",
      "â•‘                                                            â•‘\n",
      "â•‘  A multi-modal AI that SEES your writing and              â•‘\n",
      "â•‘  HEARS your pronunciation for personalized feedback       â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "============================================================\n",
      "ğŸ“š AI SENSEI - Lesson 1\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ Target: ã‚ (a)\n",
      "ğŸ“ Meaning: Vowel A\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ“ STEP 1: Write the character\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: Arial, sans-serif; padding: 10px;\">\n",
       "        <h3>âœï¸ Write: <span style=\"font-size: 48px; color: #e74c3c;\">ã‚</span></h3>\n",
       "        <canvas id=\"drawingCanvas\" width=\"280\" height=\"280\" \n",
       "                style=\"border: 3px solid #333; background: white; cursor: crosshair; border-radius: 8px;\"></canvas>\n",
       "        <br><br>\n",
       "        <button onclick=\"clearCanvas()\" style=\"padding: 10px 20px; margin: 5px; cursor: pointer; \n",
       "                background: #e74c3c; color: white; border: none; border-radius: 5px;\">ğŸ—‘ï¸ Clear</button>\n",
       "        <button onclick=\"submitDrawing()\" style=\"padding: 10px 20px; margin: 5px; cursor: pointer;\n",
       "                background: #27ae60; color: white; border: none; border-radius: 5px;\">âœ… Submit</button>\n",
       "        <p id=\"canvasStatus\" style=\"color: #666;\">Draw the character above, then click Submit.</p>\n",
       "    </div>\n",
       "    \n",
       "    <script>\n",
       "    (function() {\n",
       "        var canvas = document.getElementById('drawingCanvas');\n",
       "        var ctx = canvas.getContext('2d');\n",
       "        var drawing = false;\n",
       "        var lastX = 0, lastY = 0;\n",
       "        \n",
       "        // Set drawing style\n",
       "        ctx.strokeStyle = '#000';\n",
       "        ctx.lineWidth = 8;\n",
       "        ctx.lineCap = 'round';\n",
       "        ctx.lineJoin = 'round';\n",
       "        \n",
       "        // Fill white background\n",
       "        ctx.fillStyle = 'white';\n",
       "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
       "        \n",
       "        canvas.addEventListener('mousedown', function(e) {\n",
       "            drawing = true;\n",
       "            lastX = e.offsetX;\n",
       "            lastY = e.offsetY;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('mousemove', function(e) {\n",
       "            if (!drawing) return;\n",
       "            ctx.beginPath();\n",
       "            ctx.moveTo(lastX, lastY);\n",
       "            ctx.lineTo(e.offsetX, e.offsetY);\n",
       "            ctx.stroke();\n",
       "            lastX = e.offsetX;\n",
       "            lastY = e.offsetY;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('mouseup', function() { drawing = false; });\n",
       "        canvas.addEventListener('mouseout', function() { drawing = false; });\n",
       "        \n",
       "        // Touch support\n",
       "        canvas.addEventListener('touchstart', function(e) {\n",
       "            e.preventDefault();\n",
       "            var touch = e.touches[0];\n",
       "            var rect = canvas.getBoundingClientRect();\n",
       "            drawing = true;\n",
       "            lastX = touch.clientX - rect.left;\n",
       "            lastY = touch.clientY - rect.top;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('touchmove', function(e) {\n",
       "            e.preventDefault();\n",
       "            if (!drawing) return;\n",
       "            var touch = e.touches[0];\n",
       "            var rect = canvas.getBoundingClientRect();\n",
       "            ctx.beginPath();\n",
       "            ctx.moveTo(lastX, lastY);\n",
       "            ctx.lineTo(touch.clientX - rect.left, touch.clientY - rect.top);\n",
       "            ctx.stroke();\n",
       "            lastX = touch.clientX - rect.left;\n",
       "            lastY = touch.clientY - rect.top;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('touchend', function() { drawing = false; });\n",
       "        \n",
       "        window.clearCanvas = function() {\n",
       "            ctx.fillStyle = 'white';\n",
       "            ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
       "            document.getElementById('canvasStatus').innerText = 'Canvas cleared. Draw again!';\n",
       "        };\n",
       "        \n",
       "        window.submitDrawing = function() {\n",
       "            var dataURL = canvas.toDataURL('image/png');\n",
       "            document.getElementById('canvasStatus').innerText = 'âœ… Drawing captured!';\n",
       "            \n",
       "            // Send to Python kernel (works in Colab/Jupyter)\n",
       "            if (typeof google !== 'undefined' && google.colab) {\n",
       "                google.colab.kernel.invokeFunction('notebook.capture_image', [dataURL], {});\n",
       "            } else {\n",
       "                // For standard Jupyter, store in a global variable\n",
       "                window.capturedImageData = dataURL;\n",
       "                console.log('Image captured (length: ' + dataURL.length + ')');\n",
       "            }\n",
       "        };\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ STEP 2: Pronounce the phrase\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: Arial, sans-serif; padding: 10px;\">\n",
       "        <h3>ğŸ¤ Say: <span style=\"font-size: 32px; color: #3498db;\">ã‚</span></h3>\n",
       "        <p style=\"color: #666;\">(a)</p>\n",
       "        \n",
       "        <button id=\"recordBtn\" onclick=\"toggleRecording()\" \n",
       "                style=\"padding: 15px 30px; margin: 10px; cursor: pointer; font-size: 18px;\n",
       "                       background: #3498db; color: white; border: none; border-radius: 8px;\">\n",
       "            ğŸ™ï¸ Start Recording\n",
       "        </button>\n",
       "        \n",
       "        <div id=\"audioStatus\" style=\"margin: 10px; padding: 10px; background: #f0f0f0; border-radius: 5px;\">\n",
       "            Click the button to start recording your pronunciation.\n",
       "        </div>\n",
       "        \n",
       "        <audio id=\"audioPlayback\" controls style=\"display: none; margin-top: 10px;\"></audio>\n",
       "    </div>\n",
       "    \n",
       "    <script>\n",
       "    (function() {\n",
       "        let mediaRecorder;\n",
       "        let audioChunks = [];\n",
       "        let isRecording = false;\n",
       "        \n",
       "        window.toggleRecording = async function() {\n",
       "            const btn = document.getElementById('recordBtn');\n",
       "            const status = document.getElementById('audioStatus');\n",
       "            \n",
       "            if (!isRecording) {\n",
       "                // Start recording\n",
       "                try {\n",
       "                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
       "                    mediaRecorder = new MediaRecorder(stream);\n",
       "                    audioChunks = [];\n",
       "                    \n",
       "                    mediaRecorder.ondataavailable = event => audioChunks.push(event.data);\n",
       "                    \n",
       "                    mediaRecorder.onstop = async () => {\n",
       "                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
       "                        const audioUrl = URL.createObjectURL(audioBlob);\n",
       "                        \n",
       "                        // Show playback\n",
       "                        const playback = document.getElementById('audioPlayback');\n",
       "                        playback.src = audioUrl;\n",
       "                        playback.style.display = 'block';\n",
       "                        \n",
       "                        // Convert to base64 for Python\n",
       "                        const reader = new FileReader();\n",
       "                        reader.readAsDataURL(audioBlob);\n",
       "                        reader.onloadend = () => {\n",
       "                            const base64Audio = reader.result;\n",
       "                            status.innerHTML = 'âœ… Audio captured! (' + Math.round(audioBlob.size/1024) + ' KB)';\n",
       "                            \n",
       "                            // Send to Python kernel\n",
       "                            if (typeof google !== 'undefined' && google.colab) {\n",
       "                                google.colab.kernel.invokeFunction('notebook.capture_audio', [base64Audio], {});\n",
       "                            } else {\n",
       "                                window.capturedAudioData = base64Audio;\n",
       "                                console.log('Audio captured (size: ' + audioBlob.size + ')');\n",
       "                            }\n",
       "                        };\n",
       "                        \n",
       "                        // Stop all tracks\n",
       "                        stream.getTracks().forEach(track => track.stop());\n",
       "                    };\n",
       "                    \n",
       "                    mediaRecorder.start();\n",
       "                    isRecording = true;\n",
       "                    btn.innerHTML = 'â¹ï¸ Stop Recording';\n",
       "                    btn.style.background = '#e74c3c';\n",
       "                    status.innerHTML = 'ğŸ”´ Recording... Speak now!';\n",
       "                    \n",
       "                } catch (err) {\n",
       "                    status.innerHTML = 'âŒ Microphone access denied: ' + err.message;\n",
       "                }\n",
       "            } else {\n",
       "                // Stop recording\n",
       "                mediaRecorder.stop();\n",
       "                isRecording = false;\n",
       "                btn.innerHTML = 'ğŸ™ï¸ Start Recording';\n",
       "                btn.style.background = '#3498db';\n",
       "                status.innerHTML = 'â³ Processing audio...';\n",
       "            }\n",
       "        };\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Œ INSTRUCTIONS:\n",
      "============================================================\n",
      "1. Draw the character on the canvas above\n",
      "2. Click 'Submit' on the canvas\n",
      "3. Record your pronunciation using the microphone\n",
      "4. Run the next cell to get AI Sensei's feedback!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸš€ START LESSON - Interactive Mode\n",
    "# Run this cell to start the interactive lesson!\n",
    "\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘           ğŸŒ AI SENSEI - Japanese Tutor ğŸŒ                 â•‘\")\n",
    "print(\"â•‘                                                            â•‘\")\n",
    "print(\"â•‘  A multi-modal AI that SEES your writing and              â•‘\")\n",
    "print(\"â•‘  HEARS your pronunciation for personalized feedback       â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "\n",
    "# Display the current lesson\n",
    "sensei.display_lesson_ui()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Œ INSTRUCTIONS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Draw the character on the canvas above\")\n",
    "print(\"2. Click 'Submit' on the canvas\")\n",
    "print(\"3. Record your pronunciation using the microphone\")\n",
    "print(\"4. Run the next cell to get AI Sensei's feedback!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27bafa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your submission...\n",
      "âš ï¸ No input detected yet!\n",
      "   Please draw on the canvas and/or record audio first.\n",
      "   Then run this cell again.\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ“ PROCESS SUBMISSION - Get AI Feedback\n",
    "# Run this cell after submitting your drawing and audio!\n",
    "\n",
    "# For Colab: Register callbacks to capture data\n",
    "try:\n",
    "    from google.colab import output\n",
    "    \n",
    "    def capture_image(data):\n",
    "        captured_inputs['image_data'] = data\n",
    "        print(\"âœ… Image captured!\")\n",
    "    \n",
    "    def capture_audio(data):\n",
    "        captured_inputs['audio_data'] = data\n",
    "        print(\"âœ… Audio captured!\")\n",
    "    \n",
    "    output.register_callback('notebook.capture_image', capture_image)\n",
    "    output.register_callback('notebook.capture_audio', capture_audio)\n",
    "except:\n",
    "    pass  # Not in Colab\n",
    "\n",
    "# Process the captured inputs\n",
    "print(\"Processing your submission...\")\n",
    "\n",
    "# Check if we have data (for demo, we'll check the global JS variables)\n",
    "# In real usage, the callbacks above will populate captured_inputs\n",
    "\n",
    "if captured_inputs['image_data'] or captured_inputs['audio_data']:\n",
    "    results = sensei.process_submission(\n",
    "        image_data=captured_inputs['image_data'],\n",
    "        audio_data=captured_inputs['audio_data']\n",
    "    )\n",
    "    sensei.display_results(results)\n",
    "else:\n",
    "    print(\"âš ï¸ No input detected yet!\")\n",
    "    print(\"   Please draw on the canvas and/or record audio first.\")\n",
    "    print(\"   Then run this cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "298771fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“š AI SENSEI - Lesson 2\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ Target: ã„ (i)\n",
      "ğŸ“ Meaning: Vowel I\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ“ STEP 1: Write the character\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: Arial, sans-serif; padding: 10px;\">\n",
       "        <h3>âœï¸ Write: <span style=\"font-size: 48px; color: #e74c3c;\">ã„</span></h3>\n",
       "        <canvas id=\"drawingCanvas\" width=\"280\" height=\"280\" \n",
       "                style=\"border: 3px solid #333; background: white; cursor: crosshair; border-radius: 8px;\"></canvas>\n",
       "        <br><br>\n",
       "        <button onclick=\"clearCanvas()\" style=\"padding: 10px 20px; margin: 5px; cursor: pointer; \n",
       "                background: #e74c3c; color: white; border: none; border-radius: 5px;\">ğŸ—‘ï¸ Clear</button>\n",
       "        <button onclick=\"submitDrawing()\" style=\"padding: 10px 20px; margin: 5px; cursor: pointer;\n",
       "                background: #27ae60; color: white; border: none; border-radius: 5px;\">âœ… Submit</button>\n",
       "        <p id=\"canvasStatus\" style=\"color: #666;\">Draw the character above, then click Submit.</p>\n",
       "    </div>\n",
       "    \n",
       "    <script>\n",
       "    (function() {\n",
       "        var canvas = document.getElementById('drawingCanvas');\n",
       "        var ctx = canvas.getContext('2d');\n",
       "        var drawing = false;\n",
       "        var lastX = 0, lastY = 0;\n",
       "        \n",
       "        // Set drawing style\n",
       "        ctx.strokeStyle = '#000';\n",
       "        ctx.lineWidth = 8;\n",
       "        ctx.lineCap = 'round';\n",
       "        ctx.lineJoin = 'round';\n",
       "        \n",
       "        // Fill white background\n",
       "        ctx.fillStyle = 'white';\n",
       "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
       "        \n",
       "        canvas.addEventListener('mousedown', function(e) {\n",
       "            drawing = true;\n",
       "            lastX = e.offsetX;\n",
       "            lastY = e.offsetY;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('mousemove', function(e) {\n",
       "            if (!drawing) return;\n",
       "            ctx.beginPath();\n",
       "            ctx.moveTo(lastX, lastY);\n",
       "            ctx.lineTo(e.offsetX, e.offsetY);\n",
       "            ctx.stroke();\n",
       "            lastX = e.offsetX;\n",
       "            lastY = e.offsetY;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('mouseup', function() { drawing = false; });\n",
       "        canvas.addEventListener('mouseout', function() { drawing = false; });\n",
       "        \n",
       "        // Touch support\n",
       "        canvas.addEventListener('touchstart', function(e) {\n",
       "            e.preventDefault();\n",
       "            var touch = e.touches[0];\n",
       "            var rect = canvas.getBoundingClientRect();\n",
       "            drawing = true;\n",
       "            lastX = touch.clientX - rect.left;\n",
       "            lastY = touch.clientY - rect.top;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('touchmove', function(e) {\n",
       "            e.preventDefault();\n",
       "            if (!drawing) return;\n",
       "            var touch = e.touches[0];\n",
       "            var rect = canvas.getBoundingClientRect();\n",
       "            ctx.beginPath();\n",
       "            ctx.moveTo(lastX, lastY);\n",
       "            ctx.lineTo(touch.clientX - rect.left, touch.clientY - rect.top);\n",
       "            ctx.stroke();\n",
       "            lastX = touch.clientX - rect.left;\n",
       "            lastY = touch.clientY - rect.top;\n",
       "        });\n",
       "        \n",
       "        canvas.addEventListener('touchend', function() { drawing = false; });\n",
       "        \n",
       "        window.clearCanvas = function() {\n",
       "            ctx.fillStyle = 'white';\n",
       "            ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
       "            document.getElementById('canvasStatus').innerText = 'Canvas cleared. Draw again!';\n",
       "        };\n",
       "        \n",
       "        window.submitDrawing = function() {\n",
       "            var dataURL = canvas.toDataURL('image/png');\n",
       "            document.getElementById('canvasStatus').innerText = 'âœ… Drawing captured!';\n",
       "            \n",
       "            // Send to Python kernel (works in Colab/Jupyter)\n",
       "            if (typeof google !== 'undefined' && google.colab) {\n",
       "                google.colab.kernel.invokeFunction('notebook.capture_image', [dataURL], {});\n",
       "            } else {\n",
       "                // For standard Jupyter, store in a global variable\n",
       "                window.capturedImageData = dataURL;\n",
       "                console.log('Image captured (length: ' + dataURL.length + ')');\n",
       "            }\n",
       "        };\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ STEP 2: Pronounce the phrase\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: Arial, sans-serif; padding: 10px;\">\n",
       "        <h3>ğŸ¤ Say: <span style=\"font-size: 32px; color: #3498db;\">ã„</span></h3>\n",
       "        <p style=\"color: #666;\">(i)</p>\n",
       "        \n",
       "        <button id=\"recordBtn\" onclick=\"toggleRecording()\" \n",
       "                style=\"padding: 15px 30px; margin: 10px; cursor: pointer; font-size: 18px;\n",
       "                       background: #3498db; color: white; border: none; border-radius: 8px;\">\n",
       "            ğŸ™ï¸ Start Recording\n",
       "        </button>\n",
       "        \n",
       "        <div id=\"audioStatus\" style=\"margin: 10px; padding: 10px; background: #f0f0f0; border-radius: 5px;\">\n",
       "            Click the button to start recording your pronunciation.\n",
       "        </div>\n",
       "        \n",
       "        <audio id=\"audioPlayback\" controls style=\"display: none; margin-top: 10px;\"></audio>\n",
       "    </div>\n",
       "    \n",
       "    <script>\n",
       "    (function() {\n",
       "        let mediaRecorder;\n",
       "        let audioChunks = [];\n",
       "        let isRecording = false;\n",
       "        \n",
       "        window.toggleRecording = async function() {\n",
       "            const btn = document.getElementById('recordBtn');\n",
       "            const status = document.getElementById('audioStatus');\n",
       "            \n",
       "            if (!isRecording) {\n",
       "                // Start recording\n",
       "                try {\n",
       "                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
       "                    mediaRecorder = new MediaRecorder(stream);\n",
       "                    audioChunks = [];\n",
       "                    \n",
       "                    mediaRecorder.ondataavailable = event => audioChunks.push(event.data);\n",
       "                    \n",
       "                    mediaRecorder.onstop = async () => {\n",
       "                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
       "                        const audioUrl = URL.createObjectURL(audioBlob);\n",
       "                        \n",
       "                        // Show playback\n",
       "                        const playback = document.getElementById('audioPlayback');\n",
       "                        playback.src = audioUrl;\n",
       "                        playback.style.display = 'block';\n",
       "                        \n",
       "                        // Convert to base64 for Python\n",
       "                        const reader = new FileReader();\n",
       "                        reader.readAsDataURL(audioBlob);\n",
       "                        reader.onloadend = () => {\n",
       "                            const base64Audio = reader.result;\n",
       "                            status.innerHTML = 'âœ… Audio captured! (' + Math.round(audioBlob.size/1024) + ' KB)';\n",
       "                            \n",
       "                            // Send to Python kernel\n",
       "                            if (typeof google !== 'undefined' && google.colab) {\n",
       "                                google.colab.kernel.invokeFunction('notebook.capture_audio', [base64Audio], {});\n",
       "                            } else {\n",
       "                                window.capturedAudioData = base64Audio;\n",
       "                                console.log('Audio captured (size: ' + audioBlob.size + ')');\n",
       "                            }\n",
       "                        };\n",
       "                        \n",
       "                        // Stop all tracks\n",
       "                        stream.getTracks().forEach(track => track.stop());\n",
       "                    };\n",
       "                    \n",
       "                    mediaRecorder.start();\n",
       "                    isRecording = true;\n",
       "                    btn.innerHTML = 'â¹ï¸ Stop Recording';\n",
       "                    btn.style.background = '#e74c3c';\n",
       "                    status.innerHTML = 'ğŸ”´ Recording... Speak now!';\n",
       "                    \n",
       "                } catch (err) {\n",
       "                    status.innerHTML = 'âŒ Microphone access denied: ' + err.message;\n",
       "                }\n",
       "            } else {\n",
       "                // Stop recording\n",
       "                mediaRecorder.stop();\n",
       "                isRecording = false;\n",
       "                btn.innerHTML = 'ğŸ™ï¸ Start Recording';\n",
       "                btn.style.background = '#3498db';\n",
       "                status.innerHTML = 'â³ Processing audio...';\n",
       "            }\n",
       "        };\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title â­ï¸ NEXT LESSON - Advance to Next Phrase\n",
    "sensei.advance_lesson()\n",
    "sensei.display_lesson_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1630dc",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Demo Mode: Test Without Mic/Canvas\n",
    "\n",
    "For testing the models without interactive widgets, use these demo functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf71915e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title ğŸ§ª DEMO: Test Vision Model with KMNIST Samples\n",
    "def demo_vision_model():\n",
    "    \"\"\"Test the CNN with actual KMNIST samples.\"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    testset = torchvision.datasets.KMNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Get random samples\n",
    "    indices = np.random.choice(len(testset), 5, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        image, label = testset[idx]\n",
    "        \n",
    "        # Predict\n",
    "        result = analyze_handwriting(image)\n",
    "        \n",
    "        # Display\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "        \n",
    "        actual_char = KMNIST_LABELS[label] if label < len(KMNIST_LABELS) else '?'\n",
    "        pred_char = result['predicted_char']\n",
    "        confidence = result['confidence']\n",
    "        \n",
    "        color = 'green' if actual_char == pred_char else 'red'\n",
    "        ax.set_title(f\"Actual: {actual_char}\\nPred: {pred_char} ({confidence:.0%})\", \n",
    "                     color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"ğŸ‘ï¸ Vision Model (The Eyes) - KMNIST Test Samples\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the demo\n",
    "demo_vision_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fadae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‚ Pronunciation Scoring Demo (The Ears)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Perfect pronunciation\n",
      "   Target:  ã“ã‚“ã«ã¡ã¯ â†’ konnichiha\n",
      "   Heard:   konnichiwa\n",
      "   Score:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 90/100\n",
      "   Errors:  1 edit(s) needed\n",
      "   Details: [('replace', 8, 8)]\n",
      "\n",
      "ğŸ“ Missing 'n' sound\n",
      "   Target:  ã“ã‚“ã«ã¡ã¯ â†’ konnichiha\n",
      "   Heard:   konichiwa\n",
      "   Score:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80/100\n",
      "   Errors:  2 edit(s) needed\n",
      "   Details: [('delete', 3, 3), ('replace', 8, 7)]\n",
      "\n",
      "ğŸ“ Skipped middle syllables\n",
      "   Target:  ã“ã‚“ã«ã¡ã¯ â†’ konnichiha\n",
      "   Heard:   konowa\n",
      "   Score:   â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 40/100\n",
      "   Errors:  6 edit(s) needed\n",
      "   Details: [('replace', 3, 3), ('replace', 4, 4), ('delete', 5, 5)]...\n",
      "\n",
      "ğŸ“ Perfect pronunciation\n",
      "   Target:  ã‚ã‚ŠãŒã¨ã† â†’ arigatou\n",
      "   Heard:   arigatou\n",
      "   Score:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100/100\n",
      "   Errors:  0 edit(s) needed\n",
      "\n",
      "ğŸ“ Missing final 'u'\n",
      "   Target:  ã‚ã‚ŠãŒã¨ã† â†’ arigatou\n",
      "   Heard:   arigato\n",
      "   Score:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 87/100\n",
      "   Errors:  1 edit(s) needed\n",
      "   Details: [('delete', 7, 7)]\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ§ª DEMO: Test Pronunciation Scoring with Synthetic Audio\n",
    "def demo_audio_scoring():\n",
    "    \"\"\"Test the pronunciation scorer with simulated results.\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\"target\": \"ã“ã‚“ã«ã¡ã¯\", \"simulated_heard\": \"konnichiwa\", \"description\": \"Perfect pronunciation\"},\n",
    "        {\"target\": \"ã“ã‚“ã«ã¡ã¯\", \"simulated_heard\": \"konichiwa\", \"description\": \"Missing 'n' sound\"},\n",
    "        {\"target\": \"ã“ã‚“ã«ã¡ã¯\", \"simulated_heard\": \"konowa\", \"description\": \"Skipped middle syllables\"},\n",
    "        {\"target\": \"ã‚ã‚ŠãŒã¨ã†\", \"simulated_heard\": \"arigatou\", \"description\": \"Perfect pronunciation\"},\n",
    "        {\"target\": \"ã‚ã‚ŠãŒã¨ã†\", \"simulated_heard\": \"arigato\", \"description\": \"Missing final 'u'\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ‘‚ Pronunciation Scoring Demo (The Ears)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for case in test_cases:\n",
    "        target = case['target']\n",
    "        target_romaji = to_romaji(target)\n",
    "        heard_romaji = case['simulated_heard']\n",
    "        \n",
    "        # Calculate Levenshtein distance\n",
    "        distance = Levenshtein.distance(target_romaji, heard_romaji)\n",
    "        max_len = max(len(target_romaji), len(heard_romaji))\n",
    "        score = max(0, int((1 - distance / max_len) * 100)) if max_len > 0 else 0\n",
    "        \n",
    "        # Get edit operations\n",
    "        ops = Levenshtein.editops(target_romaji, heard_romaji)\n",
    "        \n",
    "        print(f\"\\nğŸ“ {case['description']}\")\n",
    "        print(f\"   Target:  {target} â†’ {target_romaji}\")\n",
    "        print(f\"   Heard:   {heard_romaji}\")\n",
    "        print(f\"   Score:   {'â–ˆ' * (score // 10)}{'â–‘' * (10 - score // 10)} {score}/100\")\n",
    "        print(f\"   Errors:  {len(ops)} edit(s) needed\")\n",
    "        \n",
    "        if ops:\n",
    "            print(f\"   Details: {ops[:3]}{'...' if len(ops) > 3 else ''}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "demo_audio_scoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2958128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Complete Feedback Loop Demo\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š SIMULATED SENSOR DATA:\n",
      "   ğŸ‘ï¸ Vision Score: 72/100\n",
      "   ğŸ‘‚ Audio Score: 58/100\n",
      "   ğŸ” Audio detected: 'konichiwa' instead of 'konnichiwa'\n",
      "\n",
      "ğŸ§  GENERATING AI SENSEI RESPONSE...\n",
      "----------------------------------------------------------------------\n",
      "[LLM Error: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}]\n",
      "\n",
      "Based on sensor data:\n",
      "- Handwriting: 72/100\n",
      "- Pronunciation: 58/100\n",
      "Missing 'n' sound in 'konnichiwa'\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "âœ… This demonstrates how sensor data flows to the LLM for grounded feedback!\n",
      "[LLM Error: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}]\n",
      "\n",
      "Based on sensor data:\n",
      "- Handwriting: 72/100\n",
      "- Pronunciation: 58/100\n",
      "Missing 'n' sound in 'konnichiwa'\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "âœ… This demonstrates how sensor data flows to the LLM for grounded feedback!\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ§ª DEMO: Test Complete Feedback Loop (Simulated)\n",
    "def demo_complete_loop():\n",
    "    \"\"\"Demonstrate the complete feedback loop with simulated sensor data.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ Complete Feedback Loop Demo\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Simulated sensor results\n",
    "    simulated_vision = {\n",
    "        'score': 72,\n",
    "        'predicted_char': 'ã‚',\n",
    "        'predicted_romaji': 'a',\n",
    "        'confidence': 0.72,\n",
    "        'feedback': \"Good attempt at 'ã‚'. Work on stroke clarity.\"\n",
    "    }\n",
    "    \n",
    "    simulated_audio = {\n",
    "        'score': 58,\n",
    "        'target_romaji': 'konnichiwa',\n",
    "        'actual_romaji': 'konichiwa',\n",
    "        'error_indices': [3, 4],\n",
    "        'feedback': \"Missing 'n' sound in 'konnichiwa'\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ“Š SIMULATED SENSOR DATA:\")\n",
    "    print(f\"   ğŸ‘ï¸ Vision Score: {simulated_vision['score']}/100\")\n",
    "    print(f\"   ğŸ‘‚ Audio Score: {simulated_audio['score']}/100\")\n",
    "    print(f\"   ğŸ” Audio detected: '{simulated_audio['actual_romaji']}' instead of '{simulated_audio['target_romaji']}'\")\n",
    "    \n",
    "    # Generate LLM response (or fallback)\n",
    "    print(\"\\nğŸ§  GENERATING AI SENSEI RESPONSE...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    response = get_tutor_response(\n",
    "        student_profile={'name': 'Demo Student', 'level': 'beginner', 'focus': 'Hiragana'},\n",
    "        current_phrase='ã“ã‚“ã«ã¡ã¯',\n",
    "        vision_result=simulated_vision,\n",
    "        audio_result=simulated_audio\n",
    "    )\n",
    "    \n",
    "    print(response)\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    print(\"\\nâœ… This demonstrates how sensor data flows to the LLM for grounded feedback!\")\n",
    "\n",
    "demo_complete_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ac254",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Architecture Summary & Next Steps\n",
    "\n",
    "### âœ… What We Built\n",
    "\n",
    "| Component | Implementation | Purpose |\n",
    "|-----------|----------------|---------|\n",
    "| **The Eyes** | `SenseiVisionNet` CNN | Recognizes handwritten Hiragana (49 classes) |\n",
    "| **The Ears** | Wav2Vec2 + Levenshtein | Scores pronunciation phoneme-by-phoneme |\n",
    "| **The Brain** | GPT-4o/Gemini + Prompt Injection | Dynamic, grounded tutoring feedback |\n",
    "| **UI** | HTML/JS Canvas + Audio | Interactive input capture |\n",
    "| **Loop** | `AISenseiLesson` class | Orchestrates the closed feedback loop |\n",
    "\n",
    "### ğŸ”‘ Key Innovation: Sensor Data Injection\n",
    "\n",
    "```python\n",
    "# Instead of: \"How did I do?\"\n",
    "# We send: \"Vision=72%, Audio=58%, Error at positions [3,4], heard 'konichiwa' not 'konnichiwa'\"\n",
    "```\n",
    "\n",
    "This makes the AI Tutor's feedback **specific and grounded** rather than generic.\n",
    "\n",
    "### ğŸš€ Future Enhancements\n",
    "\n",
    "1. **Stroke Order Detection**: Track drawing sequence, not just final image\n",
    "2. **Real-time Feedback**: Process audio as user speaks\n",
    "3. **Spaced Repetition**: Track weak characters for targeted review\n",
    "4. **Multi-language Support**: Extend to Katakana, Kanji, Korean Hangul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c6dd8",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ğŸ—£ï¸ Conversation Mode: Voice-Enabled Back-and-Forth\n",
    "\n",
    "This mode creates a **real conversation** with AI Sensei:\n",
    "1. **AI Speaks** â†’ Text-to-Speech gives you a sentence to practice\n",
    "2. **You Speak** â†’ Speech recognition captures your response  \n",
    "3. **AI Analyzes** â†’ Pronunciation is scored phoneme-by-phoneme\n",
    "4. **AI Responds** â†’ Feedback + next sentence (loop continues)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ AI Speaks   â”‚â”€â”€â”€â–¶â”‚ You Speak   â”‚â”€â”€â”€â–¶â”‚ AI Analyzes â”‚â”€â”€â”€â–¶â”‚ AI Responds â”‚\n",
    "â”‚ (TTS)       â”‚    â”‚ (STT)       â”‚    â”‚ (Wav2Vec2)  â”‚    â”‚ (LLM)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â–²                                                        â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43efa49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text-to-Speech dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# @title Install TTS Dependencies\n",
    "# For AI Sensei to SPEAK to you\n",
    "!pip install gTTS pyttsx3 playsound --quiet\n",
    "!pip install edge-tts nest-asyncio --quiet  # Microsoft Edge TTS (high quality Japanese) + async fix\n",
    "\n",
    "print(\"âœ… Text-to-Speech dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3298de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AI Sensei's voice initialized\n",
      "   Japanese voice: ja-JP-NanamiNeural\n",
      "   English voice: en-US-JennyNeural\n"
     ]
    }
   ],
   "source": [
    "# @title Text-to-Speech Engine (AI Sensei's Voice)\n",
    "import asyncio\n",
    "import edge_tts\n",
    "from IPython.display import Audio, display, HTML\n",
    "import tempfile\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops in Jupyter\n",
    "try:\n",
    "    nest_asyncio.apply()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Japanese voice options (Microsoft Edge TTS - high quality)\n",
    "JAPANESE_VOICES = {\n",
    "    'nanami': 'ja-JP-NanamiNeural',      # Female, natural\n",
    "    'keita': 'ja-JP-KeitaNeural',        # Male, natural\n",
    "}\n",
    "\n",
    "ENGLISH_VOICES = {\n",
    "    'jenny': 'en-US-JennyNeural',        # Female\n",
    "    'guy': 'en-US-GuyNeural',            # Male\n",
    "}\n",
    "\n",
    "class SenseiVoice:\n",
    "    \"\"\"Text-to-Speech engine for AI Sensei.\"\"\"\n",
    "    \n",
    "    def __init__(self, japanese_voice: str = 'nanami', english_voice: str = 'jenny'):\n",
    "        self.jp_voice = JAPANESE_VOICES.get(japanese_voice, JAPANESE_VOICES['nanami'])\n",
    "        self.en_voice = ENGLISH_VOICES.get(english_voice, ENGLISH_VOICES['jenny'])\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "    async def _generate_audio(self, text: str, voice: str, filename: str) -> str:\n",
    "        \"\"\"Generate audio file from text.\"\"\"\n",
    "        filepath = os.path.join(self.temp_dir, filename)\n",
    "        try:\n",
    "            communicate = edge_tts.Communicate(text, voice)\n",
    "            await communicate.save(filepath)\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ TTS generation error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _run_async(self, coro):\n",
    "        \"\"\"Run async code safely in Jupyter.\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # In Jupyter with running loop, use nest_asyncio\n",
    "                return asyncio.run(coro)\n",
    "            else:\n",
    "                return loop.run_until_complete(coro)\n",
    "        except RuntimeError:\n",
    "            # Fallback: create new event loop\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "            try:\n",
    "                return loop.run_until_complete(coro)\n",
    "            finally:\n",
    "                loop.close()\n",
    "    \n",
    "    def speak_japanese(self, text: str, display_audio: bool = True) -> str:\n",
    "        \"\"\"Speak Japanese text.\"\"\"\n",
    "        try:\n",
    "            filepath = self._run_async(\n",
    "                self._generate_audio(text, self.jp_voice, 'jp_speech.mp3')\n",
    "            )\n",
    "            if filepath and os.path.exists(filepath) and display_audio:\n",
    "                display(Audio(filepath, autoplay=True))\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Japanese speech error: {e}\")\n",
    "            print(f\"   Text to speak: {text}\")\n",
    "            return None\n",
    "    \n",
    "    def speak_english(self, text: str, display_audio: bool = True) -> str:\n",
    "        \"\"\"Speak English text (for instructions).\"\"\"\n",
    "        try:\n",
    "            filepath = self._run_async(\n",
    "                self._generate_audio(text, self.en_voice, 'en_speech.mp3')\n",
    "            )\n",
    "            if filepath and os.path.exists(filepath) and display_audio:\n",
    "                display(Audio(filepath, autoplay=True))\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ English speech error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def speak_lesson(self, japanese: str, english: str = None, romaji: str = None):\n",
    "        \"\"\"Speak a lesson phrase with optional translation.\"\"\"\n",
    "        print(f\"\\nğŸŒ AI Sensei says: {japanese}\")\n",
    "        if romaji:\n",
    "            print(f\"   Romaji: {romaji}\")\n",
    "        if english:\n",
    "            print(f\"   Meaning: {english}\")\n",
    "        print()\n",
    "        \n",
    "        # Play the Japanese audio\n",
    "        self.speak_japanese(japanese)\n",
    "\n",
    "# Install nest_asyncio if not available\n",
    "try:\n",
    "    import nest_asyncio\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'nest_asyncio', '--quiet'])\n",
    "    import nest_asyncio\n",
    "\n",
    "# Initialize the voice engine\n",
    "try:\n",
    "    sensei_voice = SenseiVoice()\n",
    "    print(\"âœ… AI Sensei's voice initialized\")\n",
    "    print(f\"   Japanese voice: {sensei_voice.jp_voice}\")\n",
    "    print(f\"   English voice: {sensei_voice.en_voice}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ TTS setup issue: {e}\")\n",
    "    sensei_voice = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7f245ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conversation scenarios loaded\n",
      "   ğŸ“š Daily Greetings (3 exchanges)\n",
      "   ğŸ“š At the Store (3 exchanges)\n",
      "   ğŸ“š At a Restaurant (3 exchanges)\n",
      "   ğŸ“š Asking for Directions (2 exchanges)\n"
     ]
    }
   ],
   "source": [
    "# @title Conversation Prompts & Dialogue System\n",
    "# Structured conversation scenarios for practice\n",
    "\n",
    "CONVERSATION_SCENARIOS = {\n",
    "    'greetings': {\n",
    "        'name': 'Daily Greetings',\n",
    "        'level': 'beginner',\n",
    "        'exchanges': [\n",
    "            {\n",
    "                'sensei': 'ã“ã‚“ã«ã¡ã¯ï¼',\n",
    "                'romaji': 'Konnichiwa!',\n",
    "                'english': 'Hello!',\n",
    "                'expected_response': 'ã“ã‚“ã«ã¡ã¯',\n",
    "                'hints': ['Say hello back!']\n",
    "            },\n",
    "            {\n",
    "                'sensei': 'ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ',\n",
    "                'romaji': 'Ogenki desu ka?',\n",
    "                'english': 'How are you?',\n",
    "                'expected_response': 'å…ƒæ°—ã§ã™',\n",
    "                'hints': ['genki desu = I am fine', 'You can also say ã¯ã„ã€å…ƒæ°—ã§ã™']\n",
    "            },\n",
    "            {\n",
    "                'sensei': 'ãŠåå‰ã¯ä½•ã§ã™ã‹ï¼Ÿ',\n",
    "                'romaji': 'Onamae wa nan desu ka?',\n",
    "                'english': 'What is your name?',\n",
    "                'expected_response': 'ç§ã®åå‰ã¯',\n",
    "                'hints': ['Watashi no namae wa [your name] desu']\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    'shopping': {\n",
    "        'name': 'At the Store',\n",
    "        'level': 'intermediate',\n",
    "        'exchanges': [\n",
    "            {\n",
    "                'sensei': 'ã„ã‚‰ã£ã—ã‚ƒã„ã¾ã›ï¼',\n",
    "                'romaji': 'Irasshaimase!',\n",
    "                'english': 'Welcome! (to a store)',\n",
    "                'expected_response': 'ã™ã¿ã¾ã›ã‚“',\n",
    "                'hints': ['Sumimasen = Excuse me (to get attention)']\n",
    "            },\n",
    "            {\n",
    "                'sensei': 'ä½•ã‚’ãŠæ¢ã—ã§ã™ã‹ï¼Ÿ',\n",
    "                'romaji': 'Nani wo osagashi desu ka?',\n",
    "                'english': 'What are you looking for?',\n",
    "                'expected_response': 'ã“ã‚Œã¯ã„ãã‚‰ã§ã™ã‹',\n",
    "                'hints': ['Kore wa ikura desu ka = How much is this?']\n",
    "            },\n",
    "            {\n",
    "                'sensei': 'åƒå††ã§ã™ã€‚',\n",
    "                'romaji': 'Sen en desu.',\n",
    "                'english': 'It is 1000 yen.',\n",
    "                'expected_response': 'ã“ã‚Œã‚’ãã ã•ã„',\n",
    "                'hints': ['Kore wo kudasai = I will take this please']\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    'restaurant': {\n",
    "        'name': 'At a Restaurant',\n",
    "        'level': 'intermediate',\n",
    "        'exchanges': [\n",
    "            {\n",
    "                'sensei': 'ã„ã‚‰ã£ã—ã‚ƒã„ã¾ã›ï¼ä½•åæ§˜ã§ã™ã‹ï¼Ÿ',\n",
    "                'romaji': 'Irasshaimase! Nanmei sama desu ka?',\n",
    "                'english': 'Welcome! How many people?',\n",
    "                'expected_response': 'ä¸€äººã§ã™',\n",
    "                'hints': ['Hitori desu = One person', 'Futari desu = Two people']\n",
    "            },\n",
    "            {\n",
    "                'sensei': 'ã”æ³¨æ–‡ã¯ï¼Ÿ',\n",
    "                'romaji': 'Gochuumon wa?',\n",
    "                'english': 'Your order?',\n",
    "                'expected_response': 'ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’ãã ã•ã„',\n",
    "                'hints': ['[food] wo kudasai = [food] please']\n",
    "            },\n",
    "            {\n",
    "                'sensei': 'ãŠä¼šè¨ˆã¯äºŒåƒå††ã§ã™ã€‚',\n",
    "                'romaji': 'Okaikei wa nisen en desu.',\n",
    "                'english': 'The bill is 2000 yen.',\n",
    "                'expected_response': 'ã”ã¡ãã†ã•ã¾ã§ã—ãŸ',\n",
    "                'hints': ['Gochisousama deshita = Thank you for the meal']\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    'directions': {\n",
    "        'name': 'Asking for Directions',\n",
    "        'level': 'intermediate',\n",
    "        'exchanges': [\n",
    "            {\n",
    "                'sensei': 'ã¯ã„ã€ã©ã†ã—ã¾ã—ãŸã‹ï¼Ÿ',\n",
    "                'romaji': 'Hai, doushimashita ka?',\n",
    "                'english': 'Yes, what happened? / Can I help you?',\n",
    "                'expected_response': 'é§…ã¯ã©ã“ã§ã™ã‹',\n",
    "                'hints': ['Eki wa doko desu ka = Where is the station?']\n",
    "            },\n",
    "            {\n",
    "                'sensei': 'ã¾ã£ã™ãè¡Œã£ã¦ã€å³ã«æ›²ãŒã£ã¦ãã ã•ã„ã€‚',\n",
    "                'romaji': 'Massugu itte, migi ni magatte kudasai.',\n",
    "                'english': 'Go straight, then turn right.',\n",
    "                'expected_response': 'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',\n",
    "                'hints': ['Arigatou gozaimasu = Thank you (polite)']\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Conversation scenarios loaded\")\n",
    "for key, scenario in CONVERSATION_SCENARIOS.items():\n",
    "    print(f\"   ğŸ“š {scenario['name']} ({len(scenario['exchanges'])} exchanges)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a87c4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conversation recorder widget ready\n"
     ]
    }
   ],
   "source": [
    "# @title Interactive Conversation Recorder Widget\n",
    "def create_conversation_recorder(exchange: dict, exchange_num: int) -> HTML:\n",
    "    \"\"\"Create an interactive widget for conversation practice.\"\"\"\n",
    "    \n",
    "    recorder_html = f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;\">\n",
    "        <h2>ğŸŒ Conversation Practice - Exchange {exchange_num}</h2>\n",
    "        \n",
    "        <div style=\"background: rgba(255,255,255,0.2); padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "            <h3>ğŸ¤– AI Sensei says:</h3>\n",
    "            <p style=\"font-size: 28px; margin: 5px 0;\">{exchange['sensei']}</p>\n",
    "            <p style=\"font-size: 16px; opacity: 0.9;\">{exchange['romaji']}</p>\n",
    "            <p style=\"font-size: 14px; opacity: 0.8;\">({exchange['english']})</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "            <h3>ğŸ’¡ Hint:</h3>\n",
    "            <p>{' | '.join(exchange.get('hints', ['Try your best!']))}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"text-align: center; margin: 20px 0;\">\n",
    "            <button id=\"convRecordBtn\" onclick=\"toggleConvRecording()\" \n",
    "                    style=\"padding: 20px 40px; font-size: 20px; cursor: pointer;\n",
    "                           background: #27ae60; color: white; border: none; border-radius: 50px;\n",
    "                           box-shadow: 0 4px 15px rgba(0,0,0,0.3);\">\n",
    "                ğŸ™ï¸ Hold to Speak\n",
    "            </button>\n",
    "        </div>\n",
    "        \n",
    "        <div id=\"convStatus\" style=\"text-align: center; padding: 10px; background: rgba(0,0,0,0.2); border-radius: 10px;\">\n",
    "            Press the button and speak your response!\n",
    "        </div>\n",
    "        \n",
    "        <audio id=\"convPlayback\" controls style=\"width: 100%; margin-top: 15px; display: none;\"></audio>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "    (function() {{\n",
    "        let mediaRecorder;\n",
    "        let audioChunks = [];\n",
    "        let isRecording = false;\n",
    "        \n",
    "        window.toggleConvRecording = async function() {{\n",
    "            const btn = document.getElementById('convRecordBtn');\n",
    "            const status = document.getElementById('convStatus');\n",
    "            \n",
    "            if (!isRecording) {{\n",
    "                try {{\n",
    "                    const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }});\n",
    "                    mediaRecorder = new MediaRecorder(stream);\n",
    "                    audioChunks = [];\n",
    "                    \n",
    "                    mediaRecorder.ondataavailable = event => audioChunks.push(event.data);\n",
    "                    \n",
    "                    mediaRecorder.onstop = async () => {{\n",
    "                        const audioBlob = new Blob(audioChunks, {{ type: 'audio/wav' }});\n",
    "                        const audioUrl = URL.createObjectURL(audioBlob);\n",
    "                        \n",
    "                        const playback = document.getElementById('convPlayback');\n",
    "                        playback.src = audioUrl;\n",
    "                        playback.style.display = 'block';\n",
    "                        \n",
    "                        const reader = new FileReader();\n",
    "                        reader.readAsDataURL(audioBlob);\n",
    "                        reader.onloadend = () => {{\n",
    "                            const base64Audio = reader.result;\n",
    "                            status.innerHTML = 'âœ… Response recorded! Run the next cell to analyze.';\n",
    "                            \n",
    "                            if (typeof google !== 'undefined' && google.colab) {{\n",
    "                                google.colab.kernel.invokeFunction('notebook.capture_conversation_audio', [base64Audio], {{}});\n",
    "                            }} else {{\n",
    "                                window.conversationAudioData = base64Audio;\n",
    "                            }}\n",
    "                        }};\n",
    "                        \n",
    "                        stream.getTracks().forEach(track => track.stop());\n",
    "                    }};\n",
    "                    \n",
    "                    mediaRecorder.start();\n",
    "                    isRecording = true;\n",
    "                    btn.innerHTML = 'ğŸ”´ Recording... Click to Stop';\n",
    "                    btn.style.background = '#e74c3c';\n",
    "                    status.innerHTML = 'ğŸ¤ Listening... Speak now!';\n",
    "                    \n",
    "                }} catch (err) {{\n",
    "                    status.innerHTML = 'âŒ Mic error: ' + err.message;\n",
    "                }}\n",
    "            }} else {{\n",
    "                mediaRecorder.stop();\n",
    "                isRecording = false;\n",
    "                btn.innerHTML = 'ğŸ™ï¸ Hold to Speak';\n",
    "                btn.style.background = '#27ae60';\n",
    "            }}\n",
    "        }};\n",
    "    }})();\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(recorder_html)\n",
    "\n",
    "print(\"âœ… Conversation recorder widget ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38dd6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conversation Session Manager ready\n"
     ]
    }
   ],
   "source": [
    "# @title Conversation Session Manager\n",
    "class ConversationSession:\n",
    "    \"\"\"\n",
    "    Manages a back-and-forth conversation with AI Sensei.\n",
    "    \n",
    "    Flow:\n",
    "    1. AI Sensei speaks a prompt (TTS)\n",
    "    2. Student responds (recorded audio)\n",
    "    3. Response is analyzed (Wav2Vec2 + Levenshtein)\n",
    "    4. AI gives feedback and continues conversation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scenario_key: str = 'greetings'):\n",
    "        self.scenario = CONVERSATION_SCENARIOS.get(scenario_key, CONVERSATION_SCENARIOS['greetings'])\n",
    "        self.current_exchange_idx = 0\n",
    "        self.conversation_history = []\n",
    "        self.scores = []\n",
    "        \n",
    "    def get_current_exchange(self) -> dict:\n",
    "        \"\"\"Get the current exchange in the conversation.\"\"\"\n",
    "        if self.current_exchange_idx >= len(self.scenario['exchanges']):\n",
    "            return None\n",
    "        return self.scenario['exchanges'][self.current_exchange_idx]\n",
    "    \n",
    "    def start_conversation(self):\n",
    "        \"\"\"Begin the conversation with AI Sensei's first line.\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸŒ CONVERSATION MODE: {self.scenario['name']}\")\n",
    "        print(f\"   Level: {self.scenario['level'].upper()}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        exchange = self.get_current_exchange()\n",
    "        if not exchange:\n",
    "            print(\"âŒ No exchanges in this scenario!\")\n",
    "            return\n",
    "        \n",
    "        # Display the exchange\n",
    "        print(f\"\\nğŸ“ Exchange {self.current_exchange_idx + 1}/{len(self.scenario['exchanges'])}\")\n",
    "        \n",
    "        # AI Sensei speaks\n",
    "        if sensei_voice:\n",
    "            sensei_voice.speak_lesson(\n",
    "                japanese=exchange['sensei'],\n",
    "                english=exchange['english'],\n",
    "                romaji=exchange['romaji']\n",
    "            )\n",
    "        else:\n",
    "            print(f\"\\nğŸ¤– AI Sensei: {exchange['sensei']}\")\n",
    "            print(f\"   ({exchange['romaji']} - {exchange['english']})\")\n",
    "        \n",
    "        # Show the recorder widget\n",
    "        display(create_conversation_recorder(exchange, self.current_exchange_idx + 1))\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"ğŸ¯ Your turn! Record your response, then run the next cell to analyze.\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    def analyze_response(self, audio_data: str = None) -> dict:\n",
    "        \"\"\"Analyze the student's spoken response.\"\"\"\n",
    "        exchange = self.get_current_exchange()\n",
    "        if not exchange:\n",
    "            return {'error': 'No active exchange'}\n",
    "        \n",
    "        # Get audio data from global if not provided\n",
    "        if audio_data is None:\n",
    "            audio_data = globals().get('conversation_audio_data') or \\\n",
    "                        (hasattr(window, 'conversationAudioData') if 'window' in dir() else None)\n",
    "        \n",
    "        # For demo, if no real audio, simulate\n",
    "        if not audio_data:\n",
    "            print(\"âš ï¸ No audio detected - using simulated analysis\")\n",
    "            # Simulate a score for demo purposes\n",
    "            import random\n",
    "            simulated_score = random.randint(50, 95)\n",
    "            result = {\n",
    "                'score': simulated_score,\n",
    "                'target_romaji': to_romaji(exchange['expected_response']),\n",
    "                'actual_romaji': to_romaji(exchange['expected_response']) if simulated_score > 70 else 'simulated',\n",
    "                'transcription': exchange['expected_response'] if simulated_score > 70 else '(simulated)',\n",
    "                'feedback': 'Good attempt!' if simulated_score > 70 else 'Keep practicing!'\n",
    "            }\n",
    "        else:\n",
    "            # Process real audio\n",
    "            try:\n",
    "                audio_array, sample_rate = process_audio_data(audio_data)\n",
    "                result = calculate_pronunciation_score(\n",
    "                    exchange['expected_response'],\n",
    "                    audio_array,\n",
    "                    sample_rate\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Audio processing error: {e}\")\n",
    "                result = {'score': 0, 'feedback': str(e)}\n",
    "        \n",
    "        # Store in history\n",
    "        self.conversation_history.append({\n",
    "            'exchange': exchange,\n",
    "            'result': result\n",
    "        })\n",
    "        self.scores.append(result.get('score', 0))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def display_feedback(self, result: dict):\n",
    "        \"\"\"Display feedback for the student's response.\"\"\"\n",
    "        exchange = self.get_current_exchange()\n",
    "        score = result.get('score', 0)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ“Š RESPONSE ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Score visualization\n",
    "        bar = 'â–ˆ' * (score // 10) + 'â–‘' * (10 - score // 10)\n",
    "        print(f\"\\nğŸ¯ Pronunciation Score: {bar} {score}/100\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ Expected: {exchange['expected_response']}\")\n",
    "        print(f\"   Romaji:   {result.get('target_romaji', 'N/A')}\")\n",
    "        print(f\"ğŸ‘‚ Heard:    {result.get('transcription', 'N/A')}\")\n",
    "        print(f\"   Romaji:   {result.get('actual_romaji', 'N/A')}\")\n",
    "        \n",
    "        # Feedback based on score\n",
    "        if score >= 85:\n",
    "            print(\"\\nğŸŒŸ ç´ æ™´ã‚‰ã—ã„ï¼(Subarashii!) Excellent pronunciation!\")\n",
    "            feedback = \"Perfect! Let's continue.\"\n",
    "        elif score >= 70:\n",
    "            print(\"\\nğŸ‘ ã„ã„ã§ã™ã­ï¼(Ii desu ne!) Good job!\")\n",
    "            feedback = \"Good attempt. Minor improvements needed.\"\n",
    "        elif score >= 50:\n",
    "            print(\"\\nğŸ“š ã‚‚ã†ä¸€åº¦ï¼(Mou ichido!) Let's try again!\")\n",
    "            feedback = f\"Focus on: {result.get('feedback', 'pronunciation')}\"\n",
    "        else:\n",
    "            print(\"\\nğŸ’ª é ‘å¼µã£ã¦ï¼(Ganbatte!) Keep trying!\")\n",
    "            feedback = f\"Let's practice more. {result.get('feedback', '')}\"\n",
    "        \n",
    "        print(f\"\\nğŸ’¬ Feedback: {feedback}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return score >= 70  # Pass threshold\n",
    "    \n",
    "    def next_exchange(self):\n",
    "        \"\"\"Move to the next exchange in the conversation.\"\"\"\n",
    "        self.current_exchange_idx += 1\n",
    "        \n",
    "        if self.current_exchange_idx >= len(self.scenario['exchanges']):\n",
    "            self.end_conversation()\n",
    "            return False\n",
    "        \n",
    "        # Continue with next exchange\n",
    "        self.start_conversation()\n",
    "        return True\n",
    "    \n",
    "    def end_conversation(self):\n",
    "        \"\"\"End the conversation and show summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ‰ CONVERSATION COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if self.scores:\n",
    "            avg_score = sum(self.scores) / len(self.scores)\n",
    "            print(f\"\\nğŸ“Š Overall Performance: {avg_score:.1f}/100\")\n",
    "            \n",
    "            bar = 'â–ˆ' * (int(avg_score) // 10) + 'â–‘' * (10 - int(avg_score) // 10)\n",
    "            print(f\"   {bar}\")\n",
    "            \n",
    "            if avg_score >= 85:\n",
    "                print(\"\\nğŸ† Outstanding! You're ready for real conversations!\")\n",
    "            elif avg_score >= 70:\n",
    "                print(\"\\nâ­ Great job! Keep practicing to improve fluency.\")\n",
    "            else:\n",
    "                print(\"\\nğŸ“š Good effort! Review and try this scenario again.\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ Exchanges completed: {len(self.conversation_history)}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "# Initialize conversation session\n",
    "conversation = ConversationSession('greetings')\n",
    "print(\"âœ… Conversation Session Manager ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9132f9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘          ğŸ—£ï¸ AI SENSEI - CONVERSATION MODE ğŸ—£ï¸                      â•‘\n",
      "â•‘                                                                    â•‘\n",
      "â•‘   Have a real back-and-forth conversation in Japanese!            â•‘\n",
      "â•‘   AI Sensei speaks â†’ You respond â†’ Get instant feedback           â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ“š Available Scenarios:\n",
      "--------------------------------------------------\n",
      "   1. Daily Greetings (beginner) - 3 exchanges\n",
      "   2. At the Store (intermediate) - 3 exchanges\n",
      "   3. At a Restaurant (intermediate) - 3 exchanges\n",
      "   4. Asking for Directions (intermediate) - 2 exchanges\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¯ Selected: Daily Greetings\n",
      "\n",
      "======================================================================\n",
      "ğŸŒ CONVERSATION MODE: Daily Greetings\n",
      "   Level: BEGINNER\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Exchange 1/3\n",
      "\n",
      "ğŸŒ AI Sensei says: ã“ã‚“ã«ã¡ã¯ï¼\n",
      "   Romaji: Konnichiwa!\n",
      "   Meaning: Hello!\n",
      "\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: Arial, sans-serif; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;\">\n",
       "        <h2>ğŸŒ Conversation Practice - Exchange 1</h2>\n",
       "        \n",
       "        <div style=\"background: rgba(255,255,255,0.2); padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
       "            <h3>ğŸ¤– AI Sensei says:</h3>\n",
       "            <p style=\"font-size: 28px; margin: 5px 0;\">ã“ã‚“ã«ã¡ã¯ï¼</p>\n",
       "            <p style=\"font-size: 16px; opacity: 0.9;\">Konnichiwa!</p>\n",
       "            <p style=\"font-size: 14px; opacity: 0.8;\">(Hello!)</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
       "            <h3>ğŸ’¡ Hint:</h3>\n",
       "            <p>Say hello back!</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"text-align: center; margin: 20px 0;\">\n",
       "            <button id=\"convRecordBtn\" onclick=\"toggleConvRecording()\" \n",
       "                    style=\"padding: 20px 40px; font-size: 20px; cursor: pointer;\n",
       "                           background: #27ae60; color: white; border: none; border-radius: 50px;\n",
       "                           box-shadow: 0 4px 15px rgba(0,0,0,0.3);\">\n",
       "                ğŸ™ï¸ Hold to Speak\n",
       "            </button>\n",
       "        </div>\n",
       "        \n",
       "        <div id=\"convStatus\" style=\"text-align: center; padding: 10px; background: rgba(0,0,0,0.2); border-radius: 10px;\">\n",
       "            Press the button and speak your response!\n",
       "        </div>\n",
       "        \n",
       "        <audio id=\"convPlayback\" controls style=\"width: 100%; margin-top: 15px; display: none;\"></audio>\n",
       "    </div>\n",
       "    \n",
       "    <script>\n",
       "    (function() {\n",
       "        let mediaRecorder;\n",
       "        let audioChunks = [];\n",
       "        let isRecording = false;\n",
       "        \n",
       "        window.toggleConvRecording = async function() {\n",
       "            const btn = document.getElementById('convRecordBtn');\n",
       "            const status = document.getElementById('convStatus');\n",
       "            \n",
       "            if (!isRecording) {\n",
       "                try {\n",
       "                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
       "                    mediaRecorder = new MediaRecorder(stream);\n",
       "                    audioChunks = [];\n",
       "                    \n",
       "                    mediaRecorder.ondataavailable = event => audioChunks.push(event.data);\n",
       "                    \n",
       "                    mediaRecorder.onstop = async () => {\n",
       "                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
       "                        const audioUrl = URL.createObjectURL(audioBlob);\n",
       "                        \n",
       "                        const playback = document.getElementById('convPlayback');\n",
       "                        playback.src = audioUrl;\n",
       "                        playback.style.display = 'block';\n",
       "                        \n",
       "                        const reader = new FileReader();\n",
       "                        reader.readAsDataURL(audioBlob);\n",
       "                        reader.onloadend = () => {\n",
       "                            const base64Audio = reader.result;\n",
       "                            status.innerHTML = 'âœ… Response recorded! Run the next cell to analyze.';\n",
       "                            \n",
       "                            if (typeof google !== 'undefined' && google.colab) {\n",
       "                                google.colab.kernel.invokeFunction('notebook.capture_conversation_audio', [base64Audio], {});\n",
       "                            } else {\n",
       "                                window.conversationAudioData = base64Audio;\n",
       "                            }\n",
       "                        };\n",
       "                        \n",
       "                        stream.getTracks().forEach(track => track.stop());\n",
       "                    };\n",
       "                    \n",
       "                    mediaRecorder.start();\n",
       "                    isRecording = true;\n",
       "                    btn.innerHTML = 'ğŸ”´ Recording... Click to Stop';\n",
       "                    btn.style.background = '#e74c3c';\n",
       "                    status.innerHTML = 'ğŸ¤ Listening... Speak now!';\n",
       "                    \n",
       "                } catch (err) {\n",
       "                    status.innerHTML = 'âŒ Mic error: ' + err.message;\n",
       "                }\n",
       "            } else {\n",
       "                mediaRecorder.stop();\n",
       "                isRecording = false;\n",
       "                btn.innerHTML = 'ğŸ™ï¸ Hold to Speak';\n",
       "                btn.style.background = '#27ae60';\n",
       "            }\n",
       "        };\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ¯ Your turn! Record your response, then run the next cell to analyze.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ—£ï¸ START CONVERSATION - Choose Your Scenario\n",
    "# Run this cell to begin a conversation!\n",
    "\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘          ğŸ—£ï¸ AI SENSEI - CONVERSATION MODE ğŸ—£ï¸                      â•‘\")\n",
    "print(\"â•‘                                                                    â•‘\")\n",
    "print(\"â•‘   Have a real back-and-forth conversation in Japanese!            â•‘\")\n",
    "print(\"â•‘   AI Sensei speaks â†’ You respond â†’ Get instant feedback           â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "\n",
    "# Choose scenario\n",
    "print(\"ğŸ“š Available Scenarios:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (key, scenario) in enumerate(CONVERSATION_SCENARIOS.items(), 1):\n",
    "    print(f\"   {i}. {scenario['name']} ({scenario['level']}) - {len(scenario['exchanges'])} exchanges\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Select scenario (change this to pick different scenarios)\n",
    "SELECTED_SCENARIO = 'greetings'  # Options: 'greetings', 'shopping', 'restaurant', 'directions'\n",
    "\n",
    "print(f\"\\nğŸ¯ Selected: {CONVERSATION_SCENARIOS[SELECTED_SCENARIO]['name']}\")\n",
    "print()\n",
    "\n",
    "# Start the conversation\n",
    "conversation = ConversationSession(SELECTED_SCENARIO)\n",
    "conversation.start_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7882a757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing your response...\n",
      "âš ï¸ No audio detected - using simulated analysis\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š RESPONSE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ Pronunciation Score: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 86/100\n",
      "\n",
      "ğŸ“ Expected: ã“ã‚“ã«ã¡ã¯\n",
      "   Romaji:   konnichiha\n",
      "ğŸ‘‚ Heard:    ã“ã‚“ã«ã¡ã¯\n",
      "   Romaji:   konnichiha\n",
      "\n",
      "ğŸŒŸ ç´ æ™´ã‚‰ã—ã„ï¼(Subarashii!) Excellent pronunciation!\n",
      "\n",
      "ğŸ’¬ Feedback: Perfect! Let's continue.\n",
      "======================================================================\n",
      "\n",
      "âœ… Great! Run the next cell to continue the conversation.\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ¯ ANALYZE MY RESPONSE\n",
    "# Run this cell AFTER recording your response!\n",
    "\n",
    "# Capture audio from JS widget (for Colab)\n",
    "try:\n",
    "    from google.colab import output\n",
    "    conversation_audio_data = None\n",
    "    \n",
    "    def capture_conversation_audio(data):\n",
    "        global conversation_audio_data\n",
    "        conversation_audio_data = data\n",
    "        print(\"âœ… Audio captured from conversation widget!\")\n",
    "    \n",
    "    output.register_callback('notebook.capture_conversation_audio', capture_conversation_audio)\n",
    "except:\n",
    "    pass  # Not in Colab\n",
    "\n",
    "# Analyze the response\n",
    "print(\"ğŸ” Analyzing your response...\")\n",
    "result = conversation.analyze_response()\n",
    "\n",
    "# Display feedback\n",
    "passed = conversation.display_feedback(result)\n",
    "\n",
    "if passed:\n",
    "    print(\"\\nâœ… Great! Run the next cell to continue the conversation.\")\n",
    "else:\n",
    "    print(\"\\nğŸ”„ Try again by re-recording, or run the next cell to continue anyway.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3fd1e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸŒ CONVERSATION MODE: Daily Greetings\n",
      "   Level: BEGINNER\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Exchange 2/3\n",
      "\n",
      "ğŸŒ AI Sensei says: ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ\n",
      "   Romaji: Ogenki desu ka?\n",
      "   Meaning: How are you?\n",
      "\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: Arial, sans-serif; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;\">\n",
       "        <h2>ğŸŒ Conversation Practice - Exchange 2</h2>\n",
       "        \n",
       "        <div style=\"background: rgba(255,255,255,0.2); padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
       "            <h3>ğŸ¤– AI Sensei says:</h3>\n",
       "            <p style=\"font-size: 28px; margin: 5px 0;\">ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ</p>\n",
       "            <p style=\"font-size: 16px; opacity: 0.9;\">Ogenki desu ka?</p>\n",
       "            <p style=\"font-size: 14px; opacity: 0.8;\">(How are you?)</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
       "            <h3>ğŸ’¡ Hint:</h3>\n",
       "            <p>genki desu = I am fine | You can also say ã¯ã„ã€å…ƒæ°—ã§ã™</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"text-align: center; margin: 20px 0;\">\n",
       "            <button id=\"convRecordBtn\" onclick=\"toggleConvRecording()\" \n",
       "                    style=\"padding: 20px 40px; font-size: 20px; cursor: pointer;\n",
       "                           background: #27ae60; color: white; border: none; border-radius: 50px;\n",
       "                           box-shadow: 0 4px 15px rgba(0,0,0,0.3);\">\n",
       "                ğŸ™ï¸ Hold to Speak\n",
       "            </button>\n",
       "        </div>\n",
       "        \n",
       "        <div id=\"convStatus\" style=\"text-align: center; padding: 10px; background: rgba(0,0,0,0.2); border-radius: 10px;\">\n",
       "            Press the button and speak your response!\n",
       "        </div>\n",
       "        \n",
       "        <audio id=\"convPlayback\" controls style=\"width: 100%; margin-top: 15px; display: none;\"></audio>\n",
       "    </div>\n",
       "    \n",
       "    <script>\n",
       "    (function() {\n",
       "        let mediaRecorder;\n",
       "        let audioChunks = [];\n",
       "        let isRecording = false;\n",
       "        \n",
       "        window.toggleConvRecording = async function() {\n",
       "            const btn = document.getElementById('convRecordBtn');\n",
       "            const status = document.getElementById('convStatus');\n",
       "            \n",
       "            if (!isRecording) {\n",
       "                try {\n",
       "                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
       "                    mediaRecorder = new MediaRecorder(stream);\n",
       "                    audioChunks = [];\n",
       "                    \n",
       "                    mediaRecorder.ondataavailable = event => audioChunks.push(event.data);\n",
       "                    \n",
       "                    mediaRecorder.onstop = async () => {\n",
       "                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
       "                        const audioUrl = URL.createObjectURL(audioBlob);\n",
       "                        \n",
       "                        const playback = document.getElementById('convPlayback');\n",
       "                        playback.src = audioUrl;\n",
       "                        playback.style.display = 'block';\n",
       "                        \n",
       "                        const reader = new FileReader();\n",
       "                        reader.readAsDataURL(audioBlob);\n",
       "                        reader.onloadend = () => {\n",
       "                            const base64Audio = reader.result;\n",
       "                            status.innerHTML = 'âœ… Response recorded! Run the next cell to analyze.';\n",
       "                            \n",
       "                            if (typeof google !== 'undefined' && google.colab) {\n",
       "                                google.colab.kernel.invokeFunction('notebook.capture_conversation_audio', [base64Audio], {});\n",
       "                            } else {\n",
       "                                window.conversationAudioData = base64Audio;\n",
       "                            }\n",
       "                        };\n",
       "                        \n",
       "                        stream.getTracks().forEach(track => track.stop());\n",
       "                    };\n",
       "                    \n",
       "                    mediaRecorder.start();\n",
       "                    isRecording = true;\n",
       "                    btn.innerHTML = 'ğŸ”´ Recording... Click to Stop';\n",
       "                    btn.style.background = '#e74c3c';\n",
       "                    status.innerHTML = 'ğŸ¤ Listening... Speak now!';\n",
       "                    \n",
       "                } catch (err) {\n",
       "                    status.innerHTML = 'âŒ Mic error: ' + err.message;\n",
       "                }\n",
       "            } else {\n",
       "                mediaRecorder.stop();\n",
       "                isRecording = false;\n",
       "                btn.innerHTML = 'ğŸ™ï¸ Hold to Speak';\n",
       "                btn.style.background = '#27ae60';\n",
       "            }\n",
       "        };\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ¯ Your turn! Record your response, then run the next cell to analyze.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# @title â­ï¸ NEXT EXCHANGE - Continue Conversation\n",
    "# Run this to hear AI Sensei's next line and respond!\n",
    "\n",
    "if not conversation.next_exchange():\n",
    "    print(\"\\nğŸŠ You've completed this conversation scenario!\")\n",
    "    print(\"   Try a different scenario by changing SELECTED_SCENARIO above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03748372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Full conversation loop ready!\n",
      "   Uncomment the last line above to run, or use the step-by-step cells.\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ”„ FULL AUTO CONVERSATION LOOP (Advanced)\n",
    "# This runs a complete conversation with automatic flow!\n",
    "\n",
    "import time\n",
    "\n",
    "def run_full_conversation(scenario_key: str = 'greetings', auto_advance: bool = False):\n",
    "    \"\"\"\n",
    "    Run a complete conversation scenario.\n",
    "    \n",
    "    In auto mode, it will automatically advance after each exchange.\n",
    "    Otherwise, it waits for user input.\n",
    "    \"\"\"\n",
    "    session = ConversationSession(scenario_key)\n",
    "    \n",
    "    print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(f\"â•‘  ğŸŒ FULL CONVERSATION: {session.scenario['name'].upper():^30} â•‘\")\n",
    "    print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    \n",
    "    exchange_count = len(session.scenario['exchanges'])\n",
    "    \n",
    "    for i in range(exchange_count):\n",
    "        exchange = session.scenario['exchanges'][i]\n",
    "        \n",
    "        print(f\"\\n{'â”€' * 70}\")\n",
    "        print(f\"ğŸ“ EXCHANGE {i + 1}/{exchange_count}\")\n",
    "        print(f\"{'â”€' * 70}\")\n",
    "        \n",
    "        # AI Sensei speaks\n",
    "        print(f\"\\nğŸ¤– AI Sensei: {exchange['sensei']}\")\n",
    "        print(f\"   ({exchange['romaji']})\")\n",
    "        print(f\"   \\\"{exchange['english']}\\\"\")\n",
    "        \n",
    "        if sensei_voice:\n",
    "            try:\n",
    "                sensei_voice.speak_japanese(exchange['sensei'])\n",
    "                time.sleep(0.5)  # Brief pause\n",
    "            except Exception as e:\n",
    "                print(f\"   [TTS unavailable: {e}]\")\n",
    "        \n",
    "        # Show expected response\n",
    "        print(f\"\\nğŸ’¬ Expected response: {exchange['expected_response']}\")\n",
    "        print(f\"   Hint: {exchange['hints'][0] if exchange.get('hints') else 'Try your best!'}\")\n",
    "        \n",
    "        # Display recorder widget\n",
    "        display(create_conversation_recorder(exchange, i + 1))\n",
    "        \n",
    "        if not auto_advance:\n",
    "            # Wait for user to record and analyze\n",
    "            input(\"\\nâ¸ï¸  Press Enter after recording to analyze (or type 'skip' to continue)...\")\n",
    "        \n",
    "        # Simulate analysis for demo\n",
    "        session.current_exchange_idx = i\n",
    "        result = session.analyze_response()\n",
    "        session.display_feedback(result)\n",
    "        \n",
    "        if auto_advance:\n",
    "            time.sleep(2)  # Auto pause between exchanges\n",
    "    \n",
    "    # End conversation\n",
    "    session.end_conversation()\n",
    "    return session\n",
    "\n",
    "# Run the conversation (set auto_advance=True for demo mode)\n",
    "# run_full_conversation('greetings', auto_advance=False)\n",
    "\n",
    "print(\"âœ… Full conversation loop ready!\")\n",
    "print(\"   Uncomment the last line above to run, or use the step-by-step cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b322849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Š Testing AI Sensei's voice...\n",
      "\n",
      "ğŸŒ ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯AIå…ˆç”Ÿã§ã™ã€‚\n",
      "   (Hello! I am AI Sensei.)\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "\n",
      "ğŸŒ æ—¥æœ¬èªã‚’ä¸€ç·’ã«å‹‰å¼·ã—ã¾ã—ã‚‡ã†ï¼\n",
      "   (Let's study Japanese together!)\n",
      "\n",
      "ğŸŒ æ—¥æœ¬èªã‚’ä¸€ç·’ã«å‹‰å¼·ã—ã¾ã—ã‚‡ã†ï¼\n",
      "   (Let's study Japanese together!)\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "\n",
      "ğŸŒ é ‘å¼µã£ã¦ãã ã•ã„ï¼\n",
      "   (Please do your best!)\n",
      "\n",
      "ğŸŒ é ‘å¼µã£ã¦ãã ã•ã„ï¼\n",
      "   (Please do your best!)\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "âš ï¸ TTS generation error: No audio was received. Please verify that your parameters are correct.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ§ª DEMO: Test TTS (AI Sensei's Voice)\n",
    "# Test the text-to-speech to hear AI Sensei speak!\n",
    "\n",
    "if sensei_voice:\n",
    "    print(\"ğŸ”Š Testing AI Sensei's voice...\")\n",
    "    print()\n",
    "    \n",
    "    # Test Japanese\n",
    "    test_phrases = [\n",
    "        (\"ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯AIå…ˆç”Ÿã§ã™ã€‚\", \"Hello! I am AI Sensei.\"),\n",
    "        (\"æ—¥æœ¬èªã‚’ä¸€ç·’ã«å‹‰å¼·ã—ã¾ã—ã‚‡ã†ï¼\", \"Let's study Japanese together!\"),\n",
    "        (\"é ‘å¼µã£ã¦ãã ã•ã„ï¼\", \"Please do your best!\"),\n",
    "    ]\n",
    "    \n",
    "    for jp, en in test_phrases:\n",
    "        print(f\"ğŸŒ {jp}\")\n",
    "        print(f\"   ({en})\")\n",
    "        try:\n",
    "            sensei_voice.speak_japanese(jp)\n",
    "            import time\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ TTS error: {e}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸ TTS not available. Run the TTS setup cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2a4fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Model save/load utilities ready\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ’¾ Save/Load Model Utilities\n",
    "import os\n",
    "\n",
    "MODEL_SAVE_PATH = './models/'\n",
    "\n",
    "def save_vision_model(model: nn.Module, filename: str = 'sensei_vision.pth'):\n",
    "    \"\"\"Save the trained CNN model.\"\"\"\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    path = os.path.join(MODEL_SAVE_PATH, filename)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'num_classes': 49\n",
    "    }, path)\n",
    "    print(f\"âœ… Model saved to {path}\")\n",
    "\n",
    "def load_vision_model(filename: str = 'sensei_vision.pth') -> nn.Module:\n",
    "    \"\"\"Load a previously trained CNN model.\"\"\"\n",
    "    path = os.path.join(MODEL_SAVE_PATH, filename)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ No saved model found at {path}\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model = SenseiVisionNet(num_classes=checkpoint.get('num_classes', 49))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"âœ… Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "# Uncomment to save the trained model:\n",
    "# save_vision_model(model_vision)\n",
    "\n",
    "print(\"ğŸ’¾ Model save/load utilities ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

